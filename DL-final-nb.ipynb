{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Install necessary dependencies"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# This cell will take time\n",
    "%pip install unsloth\n",
    "# Also get the latest nightly Unsloth!\n",
    "%pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch --upgrade\n",
    "%pip install torchvision --upgrade\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fine-tuning model\n",
    "1. Setting up model parameters\n",
    "2. Downloading the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any--> !CAN BE ADJUSTED!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.7: Fast Llama patching. Transformers = 4.46.2.\n",
      "   \\\\   /|    GPU: NVIDIA L4. Max memory: 22.168 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1+cu124. CUDA = 8.9. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load model and wrap with LoRA adapters"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.11.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Competition dataset"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'is_correct', 'answer', 'solution'],\n",
       "        num_rows: 1000000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'is_correct', 'answer', 'solution'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download and load competition dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\")\n",
    "# print and see dataset\n",
    "dataset"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Tokenization\n",
    "- ### Adding *Solution* as new token\n",
    "- ### Modifying the EOS_TOKEN (handling the None case)\n",
    "- ### Tokenizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is the radius of the circle inscribed in triangle $ABC$ if $AB = 22, AC=12,$ and $BC=14$? Express your answer in simplest radical form.', 'is_correct': True, 'answer': '3.16227766016838', 'solution': \"The circle is inscribed in a triangle, and we know the sides of the triangle.\\nTo use the inradius formula, we need to know the area of the triangle.\\nWe can use Heron's formula to calculate the area.\\n<llm-code>\\nimport math\\nfrom sympy import *\\n\\nAB, AC, BC = 22, 12, 14\\n\\n# Calculate the semiperimeter and area using Heron's formula\\ns = (AB + AC + BC) / 2\\nK = sqrt(s * (s - AB) * (s - AC) * (s - BC))\\n\\nprint(K)\\n</llm-code>\\n<llm-code-output>\\n75.8946638440411\\n</llm-code-output>\\nLet's now use the formula for the radius of the inscribed circle.\\n<llm-code>\\nr = K / s\\nprint(r)\\n</llm-code>\\n<llm-code-output>\\n3.16227766016838\\n</llm-code-output>\\nThe answer is \\\\boxed{3.16227766016838}\", 'text': \"You are a great mathematician and you are tasked with finding if an answer to a given maths question is correct or not. Your response should be 'True' if correct, otherwise 'False'. Below is Question and Answer.\\n\\n### Question:\\nWhat is the radius of the circle inscribed in triangle $ABC$ if $AB = 22, AC=12,$ and $BC=14$? Express your answer in simplest radical form.\\n\\n### Answer:\\n3.16227766016838\\n\\n### Solution:\\nThe circle is inscribed in a triangle, and we know the sides of the triangle.\\nTo use the inradius formula, we need to know the area of the triangle.\\nWe can use Heron's formula to calculate the area.\\n<llm-code>\\nimport math\\nfrom sympy import *\\n\\nAB, AC, BC = 22, 12, 14\\n\\n# Calculate the semiperimeter and area using Heron's formula\\ns = (AB + AC + BC) / 2\\nK = sqrt(s * (s - AB) * (s - AC) * (s - BC))\\n\\nprint(K)\\n</llm-code>\\n<llm-code-output>\\n75.8946638440411\\n</llm-code-output>\\nLet's now use the formula for the radius of the inscribed circle.\\n<llm-code>\\nr = K / s\\nprint(r)\\n</llm-code>\\n<llm-code-output>\\n3.16227766016838\\n</llm-code-output>\\nThe answer is \\\\boxed{3.16227766016838}\\n\\n### Output:\\nTrue\\n\\n<|end_of_text|>\", 'input_ids': [2675, 527, 264, 2294, 21651, 1122, 323, 499, 527, 51920, 449, 9455, 422, 459, 4320, 311, 264, 2728, 71808, 3488, 374, 4495, 477, 539, 13, 4718, 2077, 1288, 387, 364, 2575, 6, 422, 4495, 11, 6062, 364, 4139, 4527, 21883, 374, 16225, 323, 22559, 382, 14711, 16225, 512, 3923, 374, 279, 10801, 315, 279, 12960, 1672, 17890, 304, 22217, 400, 26484, 3, 422, 400, 1905, 284, 220, 1313, 11, 10807, 28, 717, 4884, 323, 400, 5002, 28, 975, 3, 30, 17855, 701, 4320, 304, 45648, 18336, 1376, 382, 14711, 22559, 512, 18, 13, 10674, 16367, 19274, 8953, 1987, 271, 14711, 12761, 512, 791, 12960, 374, 1672, 17890, 304, 264, 22217, 11, 323, 584, 1440, 279, 11314, 315, 279, 22217, 627, 1271, 1005, 279, 304, 27813, 15150, 11, 584, 1205, 311, 1440, 279, 3158, 315, 279, 22217, 627, 1687, 649, 1005, 6385, 263, 596, 15150, 311, 11294, 279, 3158, 627, 51379, 76, 26327, 397, 475, 7033, 198, 1527, 22176, 88, 1179, 20386, 1905, 11, 10807, 11, 18531, 284, 220, 1313, 11, 220, 717, 11, 220, 975, 271, 2, 21157, 279, 5347, 13154, 26402, 323, 3158, 1701, 6385, 263, 596, 15150, 198, 82, 284, 320, 1905, 489, 10807, 489, 18531, 8, 611, 220, 17, 198, 42, 284, 18430, 1161, 353, 320, 82, 482, 14469, 8, 353, 320, 82, 482, 10807, 8, 353, 320, 82, 482, 18531, 4489, 1374, 17155, 340, 524, 657, 76, 26327, 397, 51379, 76, 26327, 60624, 397, 2075, 13, 26227, 24491, 24344, 20945, 16, 198, 524, 657, 76, 26327, 60624, 397, 10267, 596, 1457, 1005, 279, 15150, 369, 279, 10801, 315, 279, 1672, 17890, 12960, 627, 51379, 76, 26327, 397, 81, 284, 735, 611, 274, 198, 1374, 2666, 340, 524, 657, 76, 26327, 397, 51379, 76, 26327, 60624, 397, 18, 13, 10674, 16367, 19274, 8953, 1987, 198, 524, 657, 76, 26327, 60624, 397, 791, 4320, 374, 1144, 80175, 90, 18, 13, 10674, 16367, 19274, 8953, 1987, 633, 14711, 9442, 512, 2575, 271, 128001], 'attention_masks': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import DatasetDict\n",
    "\n",
    "#preprocess to tokenize the dataset\n",
    "\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import DatasetDict\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token if tokenizer.eos_token is not None else '<|endoftext|>' # Common choice for models similar to GPT\n",
    "\n",
    "# Preprocessing prompt template\n",
    "prompt = \"\"\"You are a great mathematician and you are tasked with finding if an answer to a given maths question is correct or not. Your response should be 'True' if correct, otherwise 'False'. Below is Question and Answer.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Answer:\n",
    "{}\n",
    "\n",
    "### Solution:\n",
    "{}\n",
    "\n",
    "### Output:\n",
    "{}\n",
    "\n",
    "\"\"\"\n",
    "# Ensure EOS_TOKEN is properly set\n",
    "# EOS_TOKEN = tokenizer.eos_token if tokenizer.eos_token is not None else \"\\n\"  # Set a default end-of-sequence token if None\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Replace with your specific LLM tokenizer\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    # Extract each component from examples\n",
    "    questions = examples[\"question\"]\n",
    "    answers = examples[\"answer\"]\n",
    "    solutions = examples[\"solution\"]\n",
    "    outputs = examples[\"is_correct\"]\n",
    "    texts = []\n",
    "\n",
    "    # Iterate over each element\n",
    "    for question, answer, solution, output_val in zip(questions, answers, solutions, outputs):\n",
    "        # Convert each to string to ensure no TypeError, handle None values\n",
    "        question_str = str(question) if question is not None else \"\"\n",
    "        answer_str = str(answer) if answer is not None else \"\"\n",
    "        solution_str = str(solution) if solution is not None else \"\"\n",
    "        output_str = str(output_val) if output_val is not None else \"\"\n",
    "\n",
    "        # Format the prompt using the predefined template and append the EOS token\n",
    "        formatted_text = prompt.format(question_str, answer_str, solution_str, output_str) + EOS_TOKEN\n",
    "        texts.append(formatted_text)\n",
    "\n",
    "    return {\"text\": texts}\n",
    "\n",
    "\n",
    "\n",
    "# Apply the formatting function as a preprocessing step using map\n",
    "dataset = DatasetDict({\n",
    "    'train': dataset['train'].map(formatting_prompts_func, batched=True),\n",
    "    'test': dataset['test'].map(formatting_prompts_func, batched=True)\n",
    "})\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "def trim_tokens_after_eos(input_ids, attention_mask, eos_token_id=128001):\n",
    "    # Find the EOS token and trim both input_ids and attention_mask after it\n",
    "    try:\n",
    "        eos_index = input_ids.index(eos_token_id) + 1  # Include the EOS token itself\n",
    "        trimmed_input_ids = input_ids[:eos_index]\n",
    "        trimmed_attention_mask = attention_mask[:eos_index]\n",
    "        return trimmed_input_ids, trimmed_attention_mask\n",
    "    except ValueError:\n",
    "        # If EOS token is not found, return the original sequences\n",
    "        return input_ids, attention_mask\n",
    "\n",
    "def tokenize_func(examples):\n",
    "    # Tokenize without adding extra special tokens\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"text\"], \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=512, \n",
    "        return_tensors='pt', \n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    \n",
    "    # Lists to store trimmed input_ids and attention masks\n",
    "    trimmed_input_ids = []\n",
    "    trimmed_attention_masks = []\n",
    "    \n",
    "    # Iterate over each sequence to apply trimming\n",
    "    for seq, mask in zip(tokenized_inputs[\"input_ids\"], tokenized_inputs[\"attention_mask\"]):\n",
    "        seq_list = seq.tolist()\n",
    "        mask_list = mask.tolist()\n",
    "        \n",
    "        # Trim tokens and attention mask after EOS token\n",
    "        trimmed_seq, trimmed_mask = trim_tokens_after_eos(seq_list, mask_list, eos_token_id=128001)\n",
    "        \n",
    "        # Append results to the lists\n",
    "        trimmed_input_ids.append(trimmed_seq)\n",
    "        trimmed_attention_masks.append(trimmed_mask)\n",
    "\n",
    "    # Return the trimmed inputs and attention masks as separate fields\n",
    "    return {\n",
    "        \"input_ids\": trimmed_input_ids,\n",
    "        \"attention_masks\": trimmed_attention_masks\n",
    "    }\n",
    "\n",
    "# Apply the tokenizer to the dataset splits\n",
    "dataset = DatasetDict({\n",
    "    'train': dataset['train'].map(tokenize_func, batched=True),\n",
    "    'test': dataset['test'].map(tokenize_func, batched=True)\n",
    "})\n",
    "\n",
    "# Display the first processed row from the training set for verification\n",
    "print(dataset['train'][0])\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# SFT and Hyperparameters\n",
    "\n",
    "- ## Training Args\n",
    "- ## SFTTrainer args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "split_dataset = dataset['train'].train_test_split(test_size=0.3, seed=3407)\n",
    "train_dataset = split_dataset['train']\n",
    "val_dataset = split_dataset['test']  # Validation set\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_steps=5,\n",
    "    max_steps=10,  # Set higher for real training; keep lower for testing\n",
    "    learning_rate=5e-4,\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.0001,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    output_dir=\"outputs\",\n",
    "    report_to=\"none\",  # Disable reporting to third-party tools like WandB\n",
    ")\n",
    "\n",
    "# Training the model using SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,  # Use the train split\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=4,\n",
    "    packing=False,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# Train the model\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 700,000 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 8 | Total steps = 10\n",
      " \"-____-\"     Number of trainable parameters = 167,772,160\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:32, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.031400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.285900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.955800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.546100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.192600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.190600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.624400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.686500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.724100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.778000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=10, training_loss=2.6015373945236204, metrics={'train_runtime': 35.3598, 'train_samples_per_second': 2.262, 'train_steps_per_second': 0.283, 'total_flos': 1664484159946752.0, 'train_loss': 2.6015373945236204, 'epoch': 0.00011428571428571428})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 700,000 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 8 | Total steps = 10\n",
      " \"-____-\"     Number of trainable parameters = 167,772,160\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:31, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.682100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.761700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.481100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.472800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.388100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.267400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.235400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.369400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.225000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.233600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=10, training_loss=0.41167324036359787, metrics={'train_runtime': 34.7111, 'train_samples_per_second': 2.305, 'train_steps_per_second': 0.288, 'total_flos': 1570202061275136.0, 'train_loss': 0.41167324036359787, 'epoch': 0.00011428571428571428})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 700,000 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 8 | Total steps = 10\n",
      " \"-____-\"     Number of trainable parameters = 167,772,160\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:31, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.196900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.323300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.144100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.143300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.131900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.176300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.261400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.184300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.167000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=10, training_loss=0.1869490623474121, metrics={'train_runtime': 34.5131, 'train_samples_per_second': 2.318, 'train_steps_per_second': 0.29, 'total_flos': 1570202061275136.0, 'train_loss': 0.1869490623474121, 'epoch': 0.00011428571428571428})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 700,000 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 8 | Total steps = 10\n",
      " \"-____-\"     Number of trainable parameters = 167,772,160\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:31, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.184400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.092900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.086000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.088100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.119500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.071700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.134800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.101000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.091900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=10, training_loss=0.10872708559036255, metrics={'train_runtime': 34.9308, 'train_samples_per_second': 2.29, 'train_steps_per_second': 0.286, 'total_flos': 1570202061275136.0, 'train_loss': 0.10872708559036255, 'epoch': 0.00011428571428571428})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 700,000 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 8 | Total steps = 10\n",
      " \"-____-\"     Number of trainable parameters = 167,772,160\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:32, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.071700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.116800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.057600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.056600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.048200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.081600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.044200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.099500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.067900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.074600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=10, training_loss=0.07186593934893608, metrics={'train_runtime': 35.6906, 'train_samples_per_second': 2.241, 'train_steps_per_second': 0.28, 'total_flos': 1570202061275136.0, 'train_loss': 0.07186593934893608, 'epoch': 0.00011428571428571428})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 700,000 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 8 | Total steps = 10\n",
      " \"-____-\"     Number of trainable parameters = 167,772,160\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:32, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.047300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.097100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.058400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.031900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.038800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.042300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.054800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.049600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=10, training_loss=0.05197146870195866, metrics={'train_runtime': 35.389, 'train_samples_per_second': 2.261, 'train_steps_per_second': 0.283, 'total_flos': 1570202061275136.0, 'train_loss': 0.05197146870195866, 'epoch': 0.00011428571428571428})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 700,000 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 8 | Total steps = 10\n",
      " \"-____-\"     Number of trainable parameters = 167,772,160\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:32, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.040800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.082200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.064700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.031200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.027700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.045300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.040900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.033700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=10, training_loss=0.04101838879287243, metrics={'train_runtime': 35.4743, 'train_samples_per_second': 2.255, 'train_steps_per_second': 0.282, 'total_flos': 1570202061275136.0, 'train_loss': 0.04101838879287243, 'epoch': 0.00011428571428571428})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 700,000 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 8 | Total steps = 10\n",
      " \"-____-\"     Number of trainable parameters = 167,772,160\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:31, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.036900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.077100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.024200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.022200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.021400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.021700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.020600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.029100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.035200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.034800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=10, training_loss=0.03233228791505098, metrics={'train_runtime': 34.0277, 'train_samples_per_second': 2.351, 'train_steps_per_second': 0.294, 'total_flos': 1570202061275136.0, 'train_loss': 0.03233228791505098, 'epoch': 0.00011428571428571428})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 700,000 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 8 | Total steps = 10\n",
      " \"-____-\"     Number of trainable parameters = 167,772,160\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:31, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.038600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.053100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.023600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.025200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.018200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.029700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.031600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=10, training_loss=0.02887470945715904, metrics={'train_runtime': 34.2194, 'train_samples_per_second': 2.338, 'train_steps_per_second': 0.292, 'total_flos': 1570202061275136.0, 'train_loss': 0.02887470945715904, 'epoch': 0.00011428571428571428})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 700,000 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 8 | Total steps = 10\n",
      " \"-____-\"     Number of trainable parameters = 167,772,160\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:31, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.043200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.020900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.014800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.014900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.015600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.023700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=10, training_loss=0.022611537761986256, metrics={'train_runtime': 35.0441, 'train_samples_per_second': 2.283, 'train_steps_per_second': 0.285, 'total_flos': 1570202061275136.0, 'train_loss': 0.022611537761986256, 'epoch': 0.00011428571428571428})\n"
     ]
    }
   ],
   "source": [
    "# trainer.train() #0.026, 0.025, 0.023, 0.021, 0.0208, 0.0203, 0.0201, 0.0197, 0.0196, 0.0195, 0.0189, 0.0183, 0.0180, 0.0172, 0.0170, 0.0160, 0.0155\n",
    "\n",
    "for i in range(10):\n",
    "    print(trainer.train()) \n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference (over a fixed random sample)"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 63/63 [04:50<00:00,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy on 1000 samples: 0.3880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def validate_model_simple(model, tokenizer, val_dataset, num_samples=5000, batch_size=16):\n",
    "    # Randomly select 5000 samples from the validation dataset\n",
    "    random.seed(3407)\n",
    "    indices = random.sample(range(len(val_dataset)), num_samples)\n",
    "    val_subset = val_dataset.select(indices)\n",
    "\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(val_subset)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "\n",
    "    # Disable gradient calculations for faster inference\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, total_predictions, batch_size), desc=\"Validating\"):\n",
    "            batch_end = min(i + batch_size, total_predictions)\n",
    "            batch = val_subset[i:batch_end]\n",
    "\n",
    "            # Prepare input prompts\n",
    "            input_prompts = [\n",
    "                prompt.format(q, a, s, \"\") for q, a, s in zip(batch[\"question\"], batch[\"answer\"], batch[\"solution\"])\n",
    "            ]\n",
    "\n",
    "            # Tokenize the prompts\n",
    "            inputs = tokenizer(input_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "            # Generate outputs\n",
    "            outputs = model.generate(**inputs, max_new_tokens=5, use_cache=True)\n",
    "\n",
    "            # Decode the responses\n",
    "            responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "            # Check predictions\n",
    "            correct_predictions += sum(\n",
    "                (\"True\" in response) == true_label for response, true_label in zip(responses, batch[\"is_correct\"])\n",
    "            )\n",
    "\n",
    "            # Free memory\n",
    "            del inputs, outputs, responses\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Calculate and print accuracy\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f\"Validation Accuracy on {total_predictions} samples: {accuracy:.4f}\")\n",
    "\n",
    "# Usage\n",
    "validate_model_simple(model, tokenizer, val_dataset, num_samples=1000, batch_size=16)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference (over a specific data point)"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample inference data point\n",
    "test_dataset = dataset['test']\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Result on test dataset (batch inference)"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 1/625 [00:03<40:08,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 2/625 [00:08<43:08,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 3/625 [00:12<45:04,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|          | 4/625 [00:16<42:37,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|          | 5/625 [00:20<40:48,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|          | 6/625 [00:22<36:09,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|          | 7/625 [00:29<45:45,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|▏         | 8/625 [00:32<43:24,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|▏         | 9/625 [00:36<39:59,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|▏         | 10/625 [00:40<41:05,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|▏         | 11/625 [00:43<39:09,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|▏         | 12/625 [00:47<37:11,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|▏         | 13/625 [00:51<39:41,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|▏         | 14/625 [00:56<42:59,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|▏         | 15/625 [01:02<48:03,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|▎         | 16/625 [01:07<49:24,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|▎         | 17/625 [01:10<43:12,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|▎         | 18/625 [01:13<39:01,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|▎         | 19/625 [01:17<40:00,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|▎         | 20/625 [01:21<40:46,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|▎         | 21/625 [01:25<39:09,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|▎         | 22/625 [01:31<45:18,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|▎         | 23/625 [01:35<44:09,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|▍         | 24/625 [01:39<41:38,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|▍         | 25/625 [01:44<45:59,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|▍         | 26/625 [01:49<47:37,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|▍         | 27/625 [01:53<44:11,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|▍         | 28/625 [01:58<45:05,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|▍         | 29/625 [02:01<40:31,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|▍         | 30/625 [02:04<38:28,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|▍         | 31/625 [02:08<38:41,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|▌         | 32/625 [02:12<37:35,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|▌         | 33/625 [02:15<36:38,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "528 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|▌         | 34/625 [02:19<35:28,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|▌         | 35/625 [02:22<33:47,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|▌         | 36/625 [02:25<33:18,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|▌         | 37/625 [02:28<32:39,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|▌         | 38/625 [02:31<30:01,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|▌         | 39/625 [02:34<32:11,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|▋         | 40/625 [02:38<34:25,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|▋         | 41/625 [02:41<32:45,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "656 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|▋         | 42/625 [02:46<36:19,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "672 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|▋         | 43/625 [02:49<35:00,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "688 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|▋         | 44/625 [02:53<36:10,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|▋         | 45/625 [02:56<33:53,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|▋         | 46/625 [03:00<32:57,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "736 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|▊         | 47/625 [03:02<30:52,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "752 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|▊         | 48/625 [03:06<31:45,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|▊         | 49/625 [03:09<32:02,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|▊         | 50/625 [03:13<34:38,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|▊         | 51/625 [03:17<35:01,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "816 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|▊         | 52/625 [03:21<34:47,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "832 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|▊         | 53/625 [03:24<33:40,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "848 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   9%|▊         | 54/625 [03:29<36:21,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "864 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   9%|▉         | 55/625 [03:33<39:14,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   9%|▉         | 56/625 [03:36<35:23,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   9%|▉         | 57/625 [03:40<34:20,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   9%|▉         | 58/625 [03:43<32:20,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "928 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   9%|▉         | 59/625 [03:47<33:47,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  10%|▉         | 60/625 [03:51<34:57,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "960 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  10%|▉         | 61/625 [03:55<36:13,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  10%|▉         | 62/625 [03:59<38:13,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "992 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  10%|█         | 63/625 [04:03<37:55,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1008 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  10%|█         | 64/625 [04:07<38:07,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  10%|█         | 65/625 [04:11<37:10,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1040 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  11%|█         | 66/625 [04:16<39:54,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1056 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  11%|█         | 67/625 [04:23<47:30,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1072 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  11%|█         | 68/625 [04:26<41:32,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1088 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  11%|█         | 69/625 [04:31<42:11,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1104 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  11%|█         | 70/625 [04:36<42:21,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1120 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  11%|█▏        | 71/625 [04:40<41:34,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1136 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  12%|█▏        | 72/625 [04:43<37:49,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1152 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  12%|█▏        | 73/625 [04:47<37:44,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  12%|█▏        | 74/625 [04:50<35:20,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1184 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  12%|█▏        | 75/625 [04:53<32:47,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  12%|█▏        | 76/625 [04:56<29:54,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1216 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  12%|█▏        | 77/625 [05:00<31:29,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1232 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  12%|█▏        | 78/625 [05:04<32:40,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1248 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  13%|█▎        | 79/625 [05:07<31:33,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1264 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  13%|█▎        | 80/625 [05:16<45:38,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  13%|█▎        | 81/625 [05:20<43:36,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1296 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  13%|█▎        | 82/625 [05:24<40:31,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1312 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  13%|█▎        | 83/625 [05:28<41:16,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1328 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  13%|█▎        | 84/625 [05:32<39:53,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1344 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  14%|█▎        | 85/625 [05:39<45:12,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1360 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  14%|█▍        | 86/625 [05:43<41:53,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1376 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  14%|█▍        | 87/625 [05:46<39:01,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1392 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  14%|█▍        | 88/625 [05:52<42:16,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1408 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  14%|█▍        | 89/625 [05:56<41:26,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1424 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  14%|█▍        | 90/625 [06:00<38:59,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  15%|█▍        | 91/625 [06:05<39:45,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1456 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  15%|█▍        | 92/625 [06:09<38:00,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1472 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  15%|█▍        | 93/625 [06:12<36:41,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1488 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  15%|█▌        | 94/625 [06:15<32:05,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1504 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  15%|█▌        | 95/625 [06:20<37:09,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1520 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  15%|█▌        | 96/625 [06:24<35:00,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  16%|█▌        | 97/625 [06:28<34:52,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1552 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  16%|█▌        | 98/625 [06:31<34:06,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  16%|█▌        | 99/625 [06:39<42:28,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1584 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  16%|█▌        | 100/625 [06:46<48:52,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  16%|█▌        | 101/625 [06:52<49:27,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1616 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  16%|█▋        | 102/625 [06:55<43:33,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1632 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  16%|█▋        | 103/625 [06:59<41:15,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1648 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  17%|█▋        | 104/625 [07:04<41:54,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1664 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  17%|█▋        | 105/625 [07:08<39:58,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1680 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  17%|█▋        | 106/625 [07:12<37:00,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1696 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  17%|█▋        | 107/625 [07:15<33:58,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1712 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  17%|█▋        | 108/625 [07:18<31:34,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1728 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  17%|█▋        | 109/625 [07:21<30:01,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1744 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  18%|█▊        | 110/625 [07:25<31:08,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1760 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  18%|█▊        | 111/625 [07:30<33:14,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1776 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  18%|█▊        | 112/625 [07:33<31:24,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1792 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  18%|█▊        | 113/625 [07:37<32:00,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1808 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  18%|█▊        | 114/625 [07:40<30:32,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1824 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  18%|█▊        | 115/625 [07:44<31:38,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1840 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  19%|█▊        | 116/625 [07:49<33:34,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1856 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  19%|█▊        | 117/625 [07:52<31:06,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1872 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  19%|█▉        | 118/625 [07:55<31:02,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1888 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  19%|█▉        | 119/625 [08:00<32:37,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1904 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  19%|█▉        | 120/625 [08:03<32:18,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  19%|█▉        | 121/625 [08:08<33:56,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1936 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  20%|█▉        | 122/625 [08:11<31:21,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1952 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  20%|█▉        | 123/625 [08:14<29:52,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1968 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  20%|█▉        | 124/625 [08:20<35:16,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1984 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  20%|██        | 125/625 [08:24<34:45,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  20%|██        | 126/625 [08:28<33:56,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  20%|██        | 127/625 [08:32<33:26,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2032 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  20%|██        | 128/625 [08:34<29:14,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  21%|██        | 129/625 [08:38<30:08,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2064 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  21%|██        | 130/625 [08:42<31:50,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2080 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  21%|██        | 131/625 [08:48<35:31,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2096 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  21%|██        | 132/625 [08:53<39:06,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2112 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  21%|██▏       | 133/625 [08:57<36:36,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2128 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  21%|██▏       | 134/625 [09:01<35:11,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2144 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  22%|██▏       | 135/625 [09:04<32:23,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  22%|██▏       | 136/625 [09:08<32:01,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2176 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  22%|██▏       | 137/625 [09:12<32:53,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2192 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  22%|██▏       | 138/625 [09:18<35:27,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2208 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  22%|██▏       | 139/625 [09:22<35:28,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2224 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  22%|██▏       | 140/625 [09:26<33:46,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2240 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  23%|██▎       | 141/625 [09:29<31:29,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2256 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  23%|██▎       | 142/625 [09:32<30:19,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2272 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  23%|██▎       | 143/625 [09:36<28:45,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2288 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  23%|██▎       | 144/625 [09:39<27:53,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  23%|██▎       | 145/625 [09:43<29:44,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2320 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  23%|██▎       | 146/625 [09:52<41:26,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2336 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  24%|██▎       | 147/625 [09:54<34:53,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2352 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  24%|██▎       | 148/625 [09:58<33:25,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2368 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  24%|██▍       | 149/625 [10:01<30:13,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2384 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  24%|██▍       | 150/625 [10:07<36:16,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  24%|██▍       | 151/625 [10:11<34:59,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2416 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  24%|██▍       | 152/625 [10:16<34:27,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2432 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  24%|██▍       | 153/625 [10:22<38:23,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2448 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  25%|██▍       | 154/625 [10:26<37:39,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2464 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  25%|██▍       | 155/625 [10:32<40:45,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2480 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  25%|██▍       | 156/625 [10:37<39:57,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2496 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  25%|██▌       | 157/625 [10:43<41:17,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2512 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  25%|██▌       | 158/625 [10:47<38:21,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2528 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  25%|██▌       | 159/625 [10:51<35:51,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2544 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  26%|██▌       | 160/625 [10:55<35:01,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  26%|██▌       | 161/625 [10:59<32:42,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2576 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  26%|██▌       | 162/625 [11:04<34:49,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2592 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  26%|██▌       | 163/625 [11:09<35:39,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2608 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  26%|██▌       | 164/625 [11:15<38:26,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2624 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  26%|██▋       | 165/625 [11:19<35:55,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2640 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  27%|██▋       | 166/625 [11:23<35:37,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2656 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  27%|██▋       | 167/625 [11:28<35:36,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2672 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  27%|██▋       | 168/625 [11:32<33:37,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2688 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  27%|██▋       | 169/625 [11:36<33:49,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2704 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  27%|██▋       | 170/625 [11:39<30:39,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2720 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  27%|██▋       | 171/625 [11:42<26:53,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2736 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  28%|██▊       | 172/625 [11:45<24:53,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2752 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  28%|██▊       | 173/625 [11:49<27:46,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2768 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  28%|██▊       | 174/625 [11:53<27:02,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2784 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  28%|██▊       | 175/625 [11:56<26:02,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  28%|██▊       | 176/625 [12:00<27:17,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2816 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  28%|██▊       | 177/625 [12:04<28:15,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2832 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  28%|██▊       | 178/625 [12:08<29:47,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2848 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  29%|██▊       | 179/625 [12:13<30:13,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2864 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  29%|██▉       | 180/625 [12:17<29:49,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2880 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  29%|██▉       | 181/625 [12:20<28:53,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2896 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  29%|██▉       | 182/625 [12:24<29:41,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2912 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  29%|██▉       | 183/625 [12:30<31:58,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2928 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  29%|██▉       | 184/625 [12:35<33:32,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2944 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  30%|██▉       | 185/625 [12:38<31:23,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2960 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  30%|██▉       | 186/625 [12:42<30:09,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2976 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  30%|██▉       | 187/625 [12:47<32:37,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2992 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  30%|███       | 188/625 [12:50<29:42,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3008 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  30%|███       | 189/625 [12:55<30:19,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3024 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  30%|███       | 190/625 [12:59<29:49,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3040 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  31%|███       | 191/625 [13:07<39:12,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3056 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  31%|███       | 192/625 [13:11<35:52,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3072 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  31%|███       | 193/625 [13:16<35:05,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3088 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  31%|███       | 194/625 [13:21<34:56,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3104 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  31%|███       | 195/625 [13:24<31:19,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3120 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  31%|███▏      | 196/625 [13:27<28:28,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3136 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  32%|███▏      | 197/625 [13:32<30:28,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3152 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  32%|███▏      | 198/625 [13:34<26:47,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3168 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  32%|███▏      | 199/625 [13:40<30:22,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3184 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  32%|███▏      | 200/625 [13:44<30:27,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  32%|███▏      | 201/625 [13:48<28:57,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3216 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  32%|███▏      | 202/625 [13:52<29:13,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3232 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  32%|███▏      | 203/625 [13:56<28:28,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3248 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  33%|███▎      | 204/625 [14:03<34:55,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3264 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  33%|███▎      | 205/625 [14:10<38:45,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3280 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  33%|███▎      | 206/625 [14:13<33:41,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3296 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  33%|███▎      | 207/625 [14:17<31:36,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3312 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  33%|███▎      | 208/625 [14:25<38:19,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3328 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  33%|███▎      | 209/625 [14:28<33:21,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3344 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  34%|███▎      | 210/625 [14:33<33:10,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3360 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  34%|███▍      | 211/625 [14:36<30:16,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3376 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  34%|███▍      | 212/625 [14:42<33:09,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3392 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  34%|███▍      | 213/625 [14:45<29:32,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3408 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  34%|███▍      | 214/625 [14:49<28:39,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3424 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  34%|███▍      | 215/625 [14:53<28:08,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3440 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  35%|███▍      | 216/625 [14:57<27:16,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3456 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  35%|███▍      | 217/625 [15:00<25:29,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3472 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  35%|███▍      | 218/625 [15:06<30:05,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3488 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  35%|███▌      | 219/625 [15:10<28:36,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3504 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  35%|███▌      | 220/625 [15:14<29:26,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3520 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  35%|███▌      | 221/625 [15:18<27:38,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3536 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  36%|███▌      | 222/625 [15:27<37:05,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3552 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  36%|███▌      | 223/625 [15:30<33:23,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3568 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  36%|███▌      | 224/625 [15:34<30:44,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3584 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  36%|███▌      | 225/625 [15:40<32:41,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  36%|███▌      | 226/625 [15:46<34:39,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3616 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  36%|███▋      | 227/625 [15:51<35:20,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3632 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  36%|███▋      | 228/625 [15:54<30:50,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3648 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  37%|███▋      | 229/625 [15:57<26:36,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3664 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  37%|███▋      | 230/625 [16:03<31:21,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3680 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  37%|███▋      | 231/625 [16:08<31:12,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3696 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  37%|███▋      | 232/625 [16:11<27:59,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3712 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  37%|███▋      | 233/625 [16:14<25:09,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3728 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  37%|███▋      | 234/625 [16:18<24:59,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3744 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  38%|███▊      | 235/625 [16:22<24:46,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3760 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  38%|███▊      | 236/625 [16:25<24:23,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3776 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  38%|███▊      | 237/625 [16:30<25:58,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3792 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  38%|███▊      | 238/625 [16:33<23:41,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3808 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  38%|███▊      | 239/625 [16:37<23:49,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  38%|███▊      | 240/625 [16:40<23:45,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3840 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  39%|███▊      | 241/625 [16:44<24:34,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3856 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  39%|███▊      | 242/625 [16:48<24:05,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3872 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  39%|███▉      | 243/625 [16:51<22:32,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3888 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  39%|███▉      | 244/625 [16:55<22:48,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3904 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  39%|███▉      | 245/625 [16:59<24:17,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3920 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  39%|███▉      | 246/625 [17:03<24:05,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3936 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  40%|███▉      | 247/625 [17:06<22:23,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3952 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  40%|███▉      | 248/625 [17:10<23:00,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3968 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  40%|███▉      | 249/625 [17:15<26:32,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3984 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  40%|████      | 250/625 [17:23<33:05,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  40%|████      | 251/625 [17:26<28:44,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4016 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  40%|████      | 252/625 [17:29<25:38,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4032 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  40%|████      | 253/625 [17:33<24:20,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4048 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  41%|████      | 254/625 [17:37<25:41,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4064 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  41%|████      | 255/625 [17:41<24:59,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  41%|████      | 256/625 [17:44<23:00,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  41%|████      | 257/625 [17:49<25:02,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4112 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  41%|████▏     | 258/625 [17:53<24:40,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4128 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  41%|████▏     | 259/625 [17:57<24:13,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4144 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  42%|████▏     | 260/625 [18:01<24:05,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4160 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  42%|████▏     | 261/625 [18:04<22:58,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4176 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  42%|████▏     | 261/625 [18:08<25:18,  4.17s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 62\u001B[0m\n\u001B[1;32m     59\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSubmission file saved as \u001B[39m\u001B[38;5;132;01m{\u001B[39;00moutput_file\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     61\u001B[0m \u001B[38;5;66;03m# Usage\u001B[39;00m\n\u001B[0;32m---> 62\u001B[0m \u001B[43moptimized_batch_evaluate_full_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataset\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[11], line 38\u001B[0m, in \u001B[0;36moptimized_batch_evaluate_full_dataset\u001B[0;34m(model, tokenizer, test_dataset, batch_size, output_file)\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Generate predictions in batch\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m---> 38\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Decode responses from the generated outputs\u001B[39;00m\n\u001B[1;32m     41\u001B[0m responses \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mbatch_decode([output[input_token_len:] \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m outputs], skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/unsloth/models/llama.py:1424\u001B[0m, in \u001B[0;36m_wrap_fast_inference.<locals>._fast_generate\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1417\u001B[0m \u001B[38;5;66;03m# Set pad token\u001B[39;00m\n\u001B[1;32m   1418\u001B[0m \u001B[38;5;66;03m# old_pad_token_id = getattr(model.config, \"pad_token_id\", None)\u001B[39;00m\n\u001B[1;32m   1419\u001B[0m \u001B[38;5;66;03m# old_eos_token_id = getattr(model.config, \"eos_token_id\", None)\u001B[39;00m\n\u001B[1;32m   1420\u001B[0m \u001B[38;5;66;03m# model.config.pad_token_id = old_eos_token_id\u001B[39;00m\n\u001B[1;32m   1421\u001B[0m \n\u001B[1;32m   1422\u001B[0m \u001B[38;5;66;03m# Autocasted\u001B[39;00m\n\u001B[1;32m   1423\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautocast(device_type \u001B[38;5;241m=\u001B[39m device_type, dtype \u001B[38;5;241m=\u001B[39m dtype):\n\u001B[0;32m-> 1424\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1425\u001B[0m \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m   1427\u001B[0m \u001B[38;5;66;03m# Revert\u001B[39;00m\n\u001B[1;32m   1428\u001B[0m \u001B[38;5;66;03m# model.config.pad_token_id = old_pad_token_id\u001B[39;00m\n\u001B[1;32m   1429\u001B[0m \n\u001B[1;32m   1430\u001B[0m \u001B[38;5;66;03m# Unset a flag for generation!\u001B[39;00m\n",
      "File \u001B[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/peft/peft_model.py:1704\u001B[0m, in \u001B[0;36mPeftModelForCausalLM.generate\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1702\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_enable_peft_forward_hooks(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m   1703\u001B[0m         kwargs \u001B[38;5;241m=\u001B[39m {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspecial_peft_forward_args}\n\u001B[0;32m-> 1704\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbase_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1705\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1706\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_model\u001B[38;5;241m.\u001B[39mgenerate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/utils.py:2215\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m   2207\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[1;32m   2208\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m   2209\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_return_sequences,\n\u001B[1;32m   2210\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[1;32m   2211\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[1;32m   2212\u001B[0m     )\n\u001B[1;32m   2214\u001B[0m     \u001B[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001B[39;00m\n\u001B[0;32m-> 2215\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2216\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2217\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2218\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2219\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2220\u001B[0m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2221\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2222\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2223\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2225\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;129;01min\u001B[39;00m (GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SAMPLE, GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SEARCH):\n\u001B[1;32m   2226\u001B[0m     \u001B[38;5;66;03m# 11. prepare beam search scorer\u001B[39;00m\n\u001B[1;32m   2227\u001B[0m     beam_scorer \u001B[38;5;241m=\u001B[39m BeamSearchScorer(\n\u001B[1;32m   2228\u001B[0m         batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[1;32m   2229\u001B[0m         num_beams\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_beams,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2234\u001B[0m         max_length\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mmax_length,\n\u001B[1;32m   2235\u001B[0m     )\n",
      "File \u001B[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/utils.py:3206\u001B[0m, in \u001B[0;36mGenerationMixin._sample\u001B[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001B[0m\n\u001B[1;32m   3203\u001B[0m model_inputs\u001B[38;5;241m.\u001B[39mupdate({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_hidden_states\u001B[39m\u001B[38;5;124m\"\u001B[39m: output_hidden_states} \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states \u001B[38;5;28;01melse\u001B[39;00m {})\n\u001B[1;32m   3205\u001B[0m \u001B[38;5;66;03m# forward pass to get next token\u001B[39;00m\n\u001B[0;32m-> 3206\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m   3208\u001B[0m \u001B[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001B[39;00m\n\u001B[1;32m   3209\u001B[0m model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_model_kwargs_for_generation(\n\u001B[1;32m   3210\u001B[0m     outputs,\n\u001B[1;32m   3211\u001B[0m     model_kwargs,\n\u001B[1;32m   3212\u001B[0m     is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[1;32m   3213\u001B[0m )\n",
      "File \u001B[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/accelerate/hooks.py:170\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    168\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    169\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 170\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/unsloth/models/llama.py:926\u001B[0m, in \u001B[0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001B[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, *args, **kwargs)\u001B[0m\n\u001B[1;32m    908\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_CausalLM_fast_forward\u001B[39m(\n\u001B[1;32m    909\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    910\u001B[0m     input_ids: torch\u001B[38;5;241m.\u001B[39mLongTensor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    922\u001B[0m     \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    923\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[Tuple, CausalLMOutputWithPast]:\n\u001B[1;32m    925\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m past_key_values \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 926\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m \u001B[43mfast_forward_inference\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    927\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    928\u001B[0m \u001B[43m            \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    929\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    930\u001B[0m \u001B[43m            \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    931\u001B[0m \u001B[43m            \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    932\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    933\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    934\u001B[0m         causal_mask \u001B[38;5;241m=\u001B[39m xformers\u001B[38;5;241m.\u001B[39mattn_bias\u001B[38;5;241m.\u001B[39mLowerTriangularMask()\n",
      "File \u001B[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/unsloth/models/llama.py:879\u001B[0m, in \u001B[0;36mLlamaModel_fast_forward_inference\u001B[0;34m(self, input_ids, past_key_values, position_ids, attention_mask)\u001B[0m\n\u001B[1;32m    877\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n\u001B[1;32m    878\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m fast_rms_layernorm_inference(decoder_layer\u001B[38;5;241m.\u001B[39minput_layernorm, hidden_states)\n\u001B[0;32m--> 879\u001B[0m hidden_states, present_key_value \u001B[38;5;241m=\u001B[39m \u001B[43mLlamaAttention_fast_forward_inference\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    880\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecoder_layer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself_attn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    881\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    882\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    883\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    884\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    885\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdo_prefill\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mhasattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdecoder_layer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself_attn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpaged_attention\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    886\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    887\u001B[0m hidden_states \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m residual\n\u001B[1;32m    889\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n",
      "File \u001B[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/unsloth/models/llama.py:187\u001B[0m, in \u001B[0;36mLlamaAttention_fast_forward_inference\u001B[0;34m(self, hidden_states, past_key_value, position_ids, do_prefill, attention_mask)\u001B[0m\n\u001B[1;32m    185\u001B[0m Qn \u001B[38;5;241m=\u001B[39m fast_linear_forward(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mq_proj, Xn, out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtemp_QA[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m    186\u001B[0m Kn \u001B[38;5;241m=\u001B[39m fast_linear_forward(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mk_proj, Xn, out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtemp_KV[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m--> 187\u001B[0m Vn \u001B[38;5;241m=\u001B[39m \u001B[43mfast_linear_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mv_proj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mXn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtemp_KV\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    188\u001B[0m Qn \u001B[38;5;241m=\u001B[39m Qn\u001B[38;5;241m.\u001B[39mview(bsz, \u001B[38;5;241m1\u001B[39m, n_heads,    head_dim)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m    189\u001B[0m Kn \u001B[38;5;241m=\u001B[39m Kn\u001B[38;5;241m.\u001B[39mview(bsz, \u001B[38;5;241m1\u001B[39m, n_kv_heads, head_dim)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/unsloth/kernels/utils.py:368\u001B[0m, in \u001B[0;36mfast_linear_forward\u001B[0;34m(proj, X, temp_lora, out)\u001B[0m\n\u001B[1;32m    366\u001B[0m     out \u001B[38;5;241m=\u001B[39m fast_gemv(X, W, W_quant, out \u001B[38;5;241m=\u001B[39m out)\n\u001B[1;32m    367\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 368\u001B[0m     W \u001B[38;5;241m=\u001B[39m fast_dequantize(\u001B[43mW\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m, W_quant)\n\u001B[1;32m    369\u001B[0m     out \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmatmul(X, W, out \u001B[38;5;241m=\u001B[39m out)\n\u001B[1;32m    370\u001B[0m \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Enable faster inference with FastLanguageModel\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def optimized_batch_evaluate_full_dataset(model, tokenizer, test_dataset, batch_size=16, output_file=\"submission_full_new2.csv\"):\n",
    "    # Define start and end indices for batch processing\n",
    "    start_index = 0\n",
    "    end_index = len(test_dataset)\n",
    "\n",
    "    # Open the CSV file to write the results\n",
    "    with open(output_file, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"ID\", \"is_correct\"])  # Write headers once\n",
    "\n",
    "        # Use tqdm to track progress and process in batches\n",
    "        for i in tqdm(range(start_index, end_index, batch_size), desc=\"Processing batches\"):\n",
    "            batch_end = min(i + batch_size, end_index)\n",
    "            batch = test_dataset[i:batch_end]\n",
    "\n",
    "            # Prepare input prompts for each batch\n",
    "            input_prompts = [\n",
    "                prompt.format(q, a, s, \"\")  # Format the prompt without the output placeholder\n",
    "                for q, a, s in zip(batch['question'], batch['answer'], batch['solution'])\n",
    "            ]\n",
    "\n",
    "            # Tokenize the entire batch at once and move to GPU if available\n",
    "            inputs = tokenizer(input_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "            # Get input token length for slicing generated outputs\n",
    "            input_token_len = inputs['input_ids'].shape[1]\n",
    "\n",
    "            # Generate predictions in batch\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**inputs, max_new_tokens=5, use_cache=True)\n",
    "\n",
    "            # Decode responses from the generated outputs\n",
    "            responses = tokenizer.batch_decode([output[input_token_len:] for output in outputs], skip_special_tokens=True)\n",
    "\n",
    "            # Determine if the response contains \"True\" or \"False\" and prepare predictions for CSV\n",
    "            batch_predictions = [\n",
    "                [i + idx, \"True\" if \"True\" in response else \"False\"]\n",
    "                for idx, response in enumerate(responses)\n",
    "            ]\n",
    "\n",
    "            # Write batch predictions to CSV at once\n",
    "            writer.writerows(batch_predictions)\n",
    "\n",
    "            # Clear memory after each batch\n",
    "            del inputs, outputs, responses, batch_predictions\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Print progress information for every batch\n",
    "            print(f\"{i + batch_size} rows processed\")\n",
    "\n",
    "    print(f\"Submission file saved as {output_file}\")\n",
    "\n",
    "# Usage\n",
    "optimized_batch_evaluate_full_dataset(model, tokenizer, test_dataset)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Loss curve (training loss V steps)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training loss curve (Training loss vs max_steps)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:21:27.408453Z",
     "start_time": "2024-11-17T20:21:20.286725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%pip install matplotlib\n",
    "%pip install numpy"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\r\n",
      "  Obtaining dependency information for matplotlib from https://files.pythonhosted.org/packages/46/87/5f567fda78130a8394f9dcf3accb1b7b0c9baf0384307ef59032f5b1d17c/matplotlib-3.9.2-cp39-cp39-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading matplotlib-3.9.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (11 kB)\r\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\r\n",
      "  Obtaining dependency information for contourpy>=1.0.1 from https://files.pythonhosted.org/packages/ec/22/19f5b948367ab5260fb41d842c7a78dae645603881ea6bc39738bcfcabf6/contourpy-1.3.0-cp39-cp39-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading contourpy-1.3.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (5.4 kB)\r\n",
      "Collecting cycler>=0.10 (from matplotlib)\r\n",
      "  Obtaining dependency information for cycler>=0.10 from https://files.pythonhosted.org/packages/e7/05/c19819d5e3d95294a6f5947fb9b9629efb316b96de511b418c53d245aae6/cycler-0.12.1-py3-none-any.whl.metadata\r\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\r\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\r\n",
      "  Obtaining dependency information for fonttools>=4.22.0 from https://files.pythonhosted.org/packages/ca/83/12c26ce25df9de2d247c31f27cddd1acd08078ad18631032db6ce946f01e/fonttools-4.55.0-cp39-cp39-macosx_10_9_universal2.whl.metadata\r\n",
      "  Downloading fonttools-4.55.0-cp39-cp39-macosx_10_9_universal2.whl.metadata (164 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m164.5/164.5 kB\u001B[0m \u001B[31m3.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\r\n",
      "  Obtaining dependency information for kiwisolver>=1.3.1 from https://files.pythonhosted.org/packages/8a/be/a6ae58978772f685d48dd2e84460937761c53c4bbd84e42b0336473d9775/kiwisolver-1.4.7-cp39-cp39-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading kiwisolver-1.4.7-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.3 kB)\r\n",
      "Collecting numpy>=1.23 (from matplotlib)\r\n",
      "  Obtaining dependency information for numpy>=1.23 from https://files.pythonhosted.org/packages/96/ff/06d1aa3eeb1c614eda245c1ba4fb88c483bee6520d361641331872ac4b82/numpy-2.0.2-cp39-cp39-macosx_14_0_arm64.whl.metadata\r\n",
      "  Downloading numpy-2.0.2-cp39-cp39-macosx_14_0_arm64.whl.metadata (60 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m60.9/60.9 kB\u001B[0m \u001B[31m8.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from matplotlib) (24.2)\r\n",
      "Collecting pillow>=8 (from matplotlib)\r\n",
      "  Obtaining dependency information for pillow>=8 from https://files.pythonhosted.org/packages/35/e8/ff71a40ca8e24cfd6bb333cc4ca8cc24ebecb6942bb4ad1e5ec61f33d1b8/pillow-11.0.0-cp39-cp39-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading pillow-11.0.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (9.1 kB)\r\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\r\n",
      "  Obtaining dependency information for pyparsing>=2.3.1 from https://files.pythonhosted.org/packages/be/ec/2eb3cd785efd67806c46c13a17339708ddc346cbb684eade7a6e6f79536a/pyparsing-3.2.0-py3-none-any.whl.metadata\r\n",
      "  Downloading pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.9/site-packages (from matplotlib) (2.9.0.post0)\r\n",
      "Collecting importlib-resources>=3.2.0 (from matplotlib)\r\n",
      "  Obtaining dependency information for importlib-resources>=3.2.0 from https://files.pythonhosted.org/packages/e1/6a/4604f9ae2fa62ef47b9de2fa5ad599589d28c9fd1d335f32759813dfa91e/importlib_resources-6.4.5-py3-none-any.whl.metadata\r\n",
      "  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\r\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.21.0)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\r\n",
      "Downloading matplotlib-3.9.2-cp39-cp39-macosx_11_0_arm64.whl (7.8 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.8/7.8 MB\u001B[0m \u001B[31m42.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading contourpy-1.3.0-cp39-cp39-macosx_11_0_arm64.whl (249 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m249.3/249.3 kB\u001B[0m \u001B[31m28.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\r\n",
      "Downloading fonttools-4.55.0-cp39-cp39-macosx_10_9_universal2.whl (2.8 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.8/2.8 MB\u001B[0m \u001B[31m61.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\r\n",
      "Downloading kiwisolver-1.4.7-cp39-cp39-macosx_11_0_arm64.whl (64 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m64.3/64.3 kB\u001B[0m \u001B[31m10.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading numpy-2.0.2-cp39-cp39-macosx_14_0_arm64.whl (5.3 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.3/5.3 MB\u001B[0m \u001B[31m66.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading pillow-11.0.0-cp39-cp39-macosx_11_0_arm64.whl (3.0 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.0/3.0 MB\u001B[0m \u001B[31m64.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m106.9/106.9 kB\u001B[0m \u001B[31m11.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: pyparsing, pillow, numpy, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib\r\n",
      "Successfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.55.0 importlib-resources-6.4.5 kiwisolver-1.4.7 matplotlib-3.9.2 numpy-2.0.2 pillow-11.0.0 pyparsing-3.2.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.9/site-packages (2.0.2)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:21:49.047041Z",
     "start_time": "2024-11-17T20:21:33.074074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# losses are used directly from the training loss table\n",
    "losses = [5.031400, 5.285900, 3.955800, 2.546100, 2.192600, 1.190600, 1.624400, 1.724100, 0.778000, 0.682100, 0.761700, 0.481100,\n",
    "          0.472800, 0.388100, 0.267400, 0.235400, 0.369400, 0.225000, 0.233600, 0.196900, 0.323300, 0.144100, 0.143300, 0.131900,\n",
    "          0.176300, 0.261400, 0.184300, 0.167000, 0.117000, 0.184400, 0.092900, 0.086000, 0.088100, 0.091900, 0.081600, 0.044200,\n",
    "          0.099500, 0.067900, 0.074600, 0.047300, 0.058400, 0.037000, 0.031900, 0.049600, 0.064700, 0.031200, 0.027700, 0.021000,\n",
    "          0.025200, 0.020800, 0.018200, 0.026900\n",
    "        ]\n",
    "steps = np.arange(1, len(losses) + 1)  # corresponding steps"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:21:51.349739Z",
     "start_time": "2024-11-17T20:21:50.968209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps, losses, marker='o', linestyle='-', color='blue')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0EAAAIjCAYAAADFthA8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABee0lEQVR4nO3deXhTZd7G8Tvd2cq+tLQsgoqCoIIoKgLDooIKAiMKKuI+wggi6qCDAi6o4wJuiOO84oYi64yOoJUdBUEEBQUEZS0FBIGyljZ93j/OpLR0O0mTnKT5fq4rV5KTc05+CY+Ym2c5LmOMEQAAAABEiCinCwAAAACAYCIEAQAAAIgohCAAAAAAEYUQBAAAACCiEIIAAAAARBRCEAAAAICIQggCAAAAEFEIQQAAAAAiCiEIAAAAQEQhBAFABLntttvUqFEjn44dPXq0XC6XfwsCAMABhCAACAEul8vWbeHChU6X6ojbbrtNlStXdroM22bNmqWrr75atWrVUlxcnJKTk3XDDTdo/vz5TpcGAJDkMsYYp4sAgEj3wQcfFHj+3nvvKS0tTe+//36B7V27dlXdunV9fp/s7Gzl5uYqPj7e62NzcnKUk5OjhIQEn9/fV7fddpumT5+uI0eOBP29vWGM0e23367JkyfrggsuUN++fVWvXj1lZGRo1qxZWrVqlb7++mtdeumlTpcKABEtxukCAADSzTffXOD58uXLlZaWVmj76Y4dO6aKFSvafp/Y2Fif6pOkmJgYxcTwv42SvPjii5o8ebKGDRuml156qcDwwccee0zvv/++X75DY4xOnDihChUqlPlcABCJGA4HAGGiY8eOatGihVatWqUrrrhCFStW1KOPPipJ+ve//60ePXooOTlZ8fHxatKkiZ588km53e4C5zh9TtDWrVvlcrn0wgsv6K233lKTJk0UHx+viy66SCtXrixwbFFzglwul4YMGaLZs2erRYsWio+PV/PmzTV37txC9S9cuFBt2rRRQkKCmjRpokmTJvl9ntG0adPUunVrVahQQbVq1dLNN9+s9PT0Avvs3r1bgwYNUkpKiuLj45WUlKSePXtq69ateft89913uvLKK1WrVi1VqFBBjRs31u23317iex8/flzjxo1Ts2bN9MILLxT5uW655Ra1bdtWUvFzrCZPniyXy1WgnkaNGumaa67RF198oTZt2qhChQqaNGmSWrRooU6dOhU6R25ururXr6++ffsW2DZ+/Hg1b95cCQkJqlu3ru655x4dOHCgxM8FAOUR/6QHAGFk//79uvrqq3XjjTfq5ptvzhsaN3nyZFWuXFnDhw9X5cqVNX/+fD3++OPKzMzUP/7xj1LPO2XKFB0+fFj33HOPXC6Xnn/+efXu3Vu//fZbqb1HS5cu1cyZM3XfffepSpUqeuWVV9SnTx9t375dNWvWlCStXr1aV111lZKSkjRmzBi53W6NHTtWtWvXLvuX8j+TJ0/WoEGDdNFFF2ncuHHas2ePJkyYoK+//lqrV69WtWrVJEl9+vTRTz/9pL/+9a9q1KiR9u7dq7S0NG3fvj3vebdu3VS7dm397W9/U7Vq1bR161bNnDmz1O/hjz/+0LBhwxQdHe23z+WxceNG3XTTTbrnnnt011136eyzz1a/fv00evRo7d69W/Xq1StQy65du3TjjTfmbbvnnnvyvqP7779fW7Zs0WuvvabVq1fr66+/LlMvIQCEHQMACDmDBw82p/8V3aFDByPJvPnmm4X2P3bsWKFt99xzj6lYsaI5ceJE3raBAweahg0b5j3fsmWLkWRq1qxp/vjjj7zt//73v40k8+mnn+Zte+KJJwrVJMnExcWZzZs352374YcfjCTz6quv5m279tprTcWKFU16enretk2bNpmYmJhC5yzKwIEDTaVKlYp9/eTJk6ZOnTqmRYsW5vjx43nbP/vsMyPJPP7448YYYw4cOGAkmX/84x/FnmvWrFlGklm5cmWpdeU3YcIEI8nMmjXL1v5FfZ/GGPPOO+8YSWbLli152xo2bGgkmblz5xbYd+PGjYW+a2OMue+++0zlypXz2sWSJUuMJPPhhx8W2G/u3LlFbgeA8o7hcAAQRuLj4zVo0KBC2/PPDTl8+LD27dun9u3b69ixY9qwYUOp5+3Xr5+qV6+e97x9+/aSpN9++63UY7t06aImTZrkPW/ZsqUSExPzjnW73frqq6/Uq1cvJScn5+3XtGlTXX311aWe347vvvtOe/fu1X333Vdg4YYePXqoWbNm+u9//yvJ+p7i4uK0cOHCYoeBeXqMPvvsM2VnZ9uuITMzU5JUpUoVHz9FyRo3bqwrr7yywLazzjpL559/vqZOnZq3ze12a/r06br22mvz2sW0adNUtWpVde3aVfv27cu7tW7dWpUrV9aCBQsCUjMAhCpCEACEkfr16ysuLq7Q9p9++knXX3+9qlatqsTERNWuXTtvUYVDhw6Vet4GDRoUeO4JRHbmi5x+rOd4z7F79+7V8ePH1bRp00L7FbXNF9u2bZMknX322YVea9asWd7r8fHxeu655zRnzhzVrVtXV1xxhZ5//nnt3r07b/8OHTqoT58+GjNmjGrVqqWePXvqnXfeUVZWVok1JCYmSrJCaCA0bty4yO39+vXT119/nTf3aeHChdq7d6/69euXt8+mTZt06NAh1alTR7Vr1y5wO3LkiPbu3RuQmgEgVBGCACCMFLUa2MGDB9WhQwf98MMPGjt2rD799FOlpaXpueeek2RNiC9NcXNYjI2rKJTlWCcMGzZMv/zyi8aNG6eEhASNGjVK55xzjlavXi3JWuxh+vTpWrZsmYYMGaL09HTdfvvtat26dYlLdDdr1kyStHbtWlt1FLcgxOmLWXgUtxJcv379ZIzRtGnTJEmffPKJqlatqquuuipvn9zcXNWpU0dpaWlF3saOHWurZgAoLwhBABDmFi5cqP3792vy5MkaOnSorrnmGnXp0qXA8DYn1alTRwkJCdq8eXOh14ra5ouGDRtKshYPON3GjRvzXvdo0qSJHnzwQX355Zdat26dTp48qRdffLHAPpdccomefvppfffdd/rwww/1008/6eOPPy62hssvv1zVq1fXRx99VGyQyc/z53Pw4MEC2z29VnY1btxYbdu21dSpU5WTk6OZM2eqV69eBa4F1aRJE+3fv1+XXXaZunTpUujWqlUrr94TAMIdIQgAwpynJyZ/z8vJkyf1xhtvOFVSAdHR0erSpYtmz56tXbt25W3fvHmz5syZ45f3aNOmjerUqaM333yzwLC1OXPmaP369erRo4ck67pKJ06cKHBskyZNVKVKlbzjDhw4UKgX6/zzz5ekEofEVaxYUY888ojWr1+vRx55pMiesA8++EArVqzIe19JWrx4cd7rR48e1bvvvmv3Y+fp16+fli9frv/7v//Tvn37CgyFk6QbbrhBbrdbTz75ZKFjc3JyCgUxACjvWCIbAMLcpZdequrVq2vgwIG6//775XK59P7774fUcLTRo0fryy+/1GWXXaa//OUvcrvdeu2119SiRQutWbPG1jmys7P11FNPFdpeo0YN3XfffXruuec0aNAgdejQQTfddFPeEtmNGjXSAw88IEn65Zdf1LlzZ91www0699xzFRMTo1mzZmnPnj15y0m/++67euONN3T99derSZMmOnz4sP75z38qMTFR3bt3L7HGhx56SD/99JNefPFFLViwQH379lW9evW0e/duzZ49WytWrNA333wjSerWrZsaNGigO+64Qw899JCio6P1f//3f6pdu7a2b9/uxbdrhZwRI0ZoxIgRqlGjhrp06VLg9Q4dOuiee+7RuHHjtGbNGnXr1k2xsbHatGmTpk2bpgkTJhS4phAAlHeEIAAIczVr1tRnn32mBx98UH//+99VvXp13XzzzercuXOh1cSc0rp1a82ZM0cjRozQqFGjlJqaqrFjx2r9+vW2Vq+TrN6tUaNGFdrepEkT3XfffbrttttUsWJFPfvss3rkkUdUqVIlXX/99XruuefyVnxLTU3VTTfdpHnz5un9999XTEyMmjVrpk8++UR9+vSRZAWGFStW6OOPP9aePXtUtWpVtW3bVh9++GGxixN4REVF6b333lPPnj311ltv6YUXXlBmZqZq166dtwhDu3btJEmxsbGaNWuW7rvvPo0aNUr16tXTsGHDVL169SJXACxJSkqKLr30Un399de68847i7zmz5tvvqnWrVtr0qRJevTRRxUTE6NGjRrp5ptv1mWXXebV+wFAuHOZUPqnQgBAROnVq5d++uknbdq0yelSAAARhDlBAICgOH78eIHnmzZt0ueff66OHTs6UxAAIGLREwQACIqkpCTddtttOuOMM7Rt2zZNnDhRWVlZWr16tc4880ynywMARBDmBAEAguKqq67SRx99pN27dys+Pl7t2rXTM888QwACAAQdPUEAAAAAIgpzggAAAABEFEIQAAAAgIgS1nOCcnNztWvXLlWpUkUul8vpcgAAAAA4xBijw4cPKzk5WVFRJff1hHUI2rVrl1JTU50uAwAAAECI2LFjh1JSUkrcJ6xDUJUqVSRZHzQxMdFv583OztaXX36pbt26FXnVbUCincAe2gnsoJ3ADtoJ7IjkdpKZmanU1NS8jFCSsA5BniFwiYmJfg9BFStWVGJiYsQ1HthHO4EdtBPYQTuBHbQT2EE7ka1pMiyMAAAAACCiEIIAAAAARBRCEAAAAICIQggCAAAAEFEIQQAAAAAiCiEIAAAAQEQhBAEAAACIKIQgAAAAABGFEAQAAAAgohCCAAAAAEQUQhAAAACAiEIIAgAAABBRCEEAAAAAIkqM0wVEMrdbWrJEysiQkpKk9u2l6GinqwIAAADKN0KQQ2bOlIYOlXbuPLUtJUWaMEHq3du5ugAAAIDyjuFwDpg5U+rbt2AAkqT0dGv7zJnO1AUAAABEAkJQkLndVg+QMYVf82wbNszaDwAAAID/EYKCbMmSwj1A+Rkj7dhh7QcAAADA/whBQZaR4d/9AAAAAHiHEBRkSUn+3Q8AAACAdwhBQda+vZScXPzrLpeUmmrtBwAAAMD/CEFBFh0tNWlS9Gsul3U/fjzXCwIAAAAChRAUZDNmWIseuFxSnToFX0tJkaZP5zpBAAAAQCARgoJo717p3nutxyNHSrt2SZ98Yj2PipI2bSIAAQAAAIFGCAoSY6R77pH27ZNatpQef9wa8tanjxQfL+XmsiIcAAAAEAyEoCD54ANp9mwpNlZ67z0r+EhWD1CjRtbj335zqjoAAAAgchCCgmDnTumvf7Uejx4ttWpV8PXGja37LVuCWhYAAAAQkQhBAWaMdMcd0qFDUtu20sMPF96HEAQAAAAEDyEowCZNkr78UkpIkN59V4qJKbzPGWdY9wyHAwAAAAKPEBRAv/4qjRhhPX72WalZs6L3oycIAAAACB5CUIC43dKgQdLRo1KHDqfmBBXFE4LoCQIAAAACjxAUIOPHWxdFrVxZeucdaxW44niGw+3da4UmAAAAAIFDCAqAn3+WHnvMevzSS6d6eopTrZp1k6StWwNYGAAAAABCkD+43dLChdJHH0lffSXdequUlSVdfbV05532zsGQOAAAACA4ilirDN6YOVMaOtS6FlB+lSpJb78tuVz2znPGGdLq1SyOAAAAAAQaPUFlMHOm1Ldv4QAkWXN7li+3fy5WiAMAAACCgxDkI7fb6gEypujXXS5p2DBrPzsYDgcAAAAEh6MhaPTo0XK5XAVuzYq7mE6IWbKk6B4gD2OkHTus/ezwrBBHTxAAAAAQWI7PCWrevLm++uqrvOcxMY6XZEtGhn/3yz8czhj7c4kAAAAAeMfxxBETE6N69eo5XYbXkpL8u1/Dhtb9kSPSvn1S7dq+1QUAAACgZI6HoE2bNik5OVkJCQlq166dxo0bpwYNGhS5b1ZWlrKysvKeZ2ZmSpKys7OVnZ3tt5o85yrpnJdcItWvH6NduyRjCnfbuFxG9etLl1ySIzulRUdb50tPd2nTphxVq1bMZCOEDDvtBKCdwA7aCeygncCOSG4n3nxmlzHFTe0PvDlz5ujIkSM6++yzlZGRoTFjxig9PV3r1q1TlSpVCu0/evRojRkzptD2KVOmqGLFisEouYBly5L03HMX/e9Z/iBkfaWPPLJS7drZHA8naeTIy7V+fU2NGLFSl1++y3+FAgAAAOXcsWPH1L9/fx06dEiJiYkl7utoCDrdwYMH1bBhQ7300ku64447Cr1eVE9Qamqq9u3bV+oH9UZ2drbS0tLUtWtXxcbGlrjvrFkuDR8erfT0UyEoJcXoxRfduv56777a22+P1gcfROnJJ9165JFcn2pH8HjTThC5aCewg3YCO2gnsCOS20lmZqZq1aplKwQ5Phwuv2rVqumss87S5s2bi3w9Pj5e8fHxhbbHxsYG5A/ZznlvuEHq08daBS4jw5oD1L69S9HR3n+1TZpY99u3Rys2NtqXkuGAQLU/lC+0E9hBO4EdtBPYEYntxJvPG1Ih6MiRI/r11191yy23OF2KV6KjpY4dy34eLpgKAAAABJ6j1wkaMWKEFi1apK1bt+qbb77R9ddfr+joaN10001OluUYz7WCuGAqAAAAEDiO9gTt3LlTN910k/bv36/atWvr8ssv1/Lly1U7QteH9vQEbd8uud1WDxMAAAAA/3I0BH388cdOvn3ISU6W4uKkkyelnTtPXTsIAAAAgP84OhwOBUVFSY0aWY8ZEgcAAAAEBiEoxLA4AgAAABBYhKAQ4wlB9AQBAAAAgUEICjGeFeLoCQIAAAACgxAUYhgOBwAAAAQWISjEMBwOAAAACCxCUIjxDIfbs0c6dszZWgAAAIDyiBAUYqpXl6pWtR5v3epoKQAAAEC5RAgKQQyJAwAAAAKHEBSCWCEOAAAACBxCUAhihTgAAAAgcAhBIYjhcAAAAEDgEIJCEMPhAAAAgMAhBIWg/MPhjHG2FgAAAKC8IQSFoEaNrPvDh6X9+x0tBQAAACh3CEEhKCFBSk62HjMkDgAAAPAvQlCIYoU4AAAAIDAIQSHKszgCK8QBAAAA/kUIClH0BAEAAACBQQgKUYQgAAAAIDAIQSGK4XAAAABAYBCCQpSnJ2jbNsntdrYWAAAAoDwhBIWo5GQpNlbKyZF27nS6GgAAAKD8IASFqOjoUxdNZV4QAAAA4D+EoBDG4ggAAACA/xGCQpgnBLE4AgAAAOA/hKAQ5lkhjp4gAAAAwH8IQSGM4XAAAACA/xGCQhjD4QAAAAD/IwSFMM9wuN27pePHna0FAAAAKC8IQSGsenUpMdF6vHWro6UAAAAA5QYhKIS5XAyJAwAAAPyNEBTiWCEOAAAA8C9CUIhjhTgAAADAvwhBIY7hcAAAAIB/EYJCHMPhAAAAAP8iBIW4/MPhjHG2FgAAAKA8IASFuEaNrPvMTOmPPxwtBQAAACgXCEEhrkIFKSnJesyQOAAAAKDsCEFhgMURAAAAAP8hBIUBFkcAAAAA/IcQFAa4VhAAAADgP4SgMMBwOAAAAMB/CEFhgOFwAAAAgP8QgsKApydo2zbJ7Xa2FgAAACDcEYLCQP36UmyslJ0tpac7XQ0AAAAQ3ghBYSA6WmrY0HrMkDgAAACgbAhBYYIV4gAAAAD/IASFCVaIAwAAAPyDEBQmWCEOAAAA8A9CUJhgOBwAAADgH4SgMMFwOAAAAMA/CEFhwjMcLiNDOn7c2VoAAACAcEYIChM1akhVqliPt21zthYAAAAgnBGCwoTLxZA4AAAAwB8IQWGEFeIAAACAsiMEhRFWiAMAAADKjhAURhgOBwAAAJQdISiMMBwOAAAAKDtCUBjJ3xNkjLO1AAAAAOGKEBRGGjWy7jMzpQMHHC0FAAAACFuEoDBSsaJUr571mCFxAAAAgG8IQWGGxREAAACAsiEEhRkWRwAAAADKhhAUZrhWEAAAAFA2hKAww3A4AAAAoGwIQWGG4XAAAABA2RCCwoynJ2jbNsntdrYWAAAAIByFTAh69tln5XK5NGzYMKdLCWkpKVJMjHTypLRrl9PVAAAAAOEnJELQypUrNWnSJLVs2dLpUkJedLTUsKH1mCFxAAAAgPccD0FHjhzRgAED9M9//lPVq1d3upywwApxAAAAgO9inC5g8ODB6tGjh7p06aKnnnqqxH2zsrKUlZWV9zwzM1OSlJ2drezsbL/V5DmXP8/pT6mp0ZKiNHt2rurXz9XllxtFRztdVeQJ9XaC0EA7gR20E9hBO4EdkdxOvPnMLmOMCWAtJfr444/19NNPa+XKlUpISFDHjh11/vnna/z48UXuP3r0aI0ZM6bQ9ilTpqhixYoBrjY0LFuWpFdfvUDHjsXmbatZ87juvHOt2rXLcLAyAAAAwDnHjh1T//79dejQISUmJpa4r2MhaMeOHWrTpo3S0tLy5gKVFoKK6glKTU3Vvn37Sv2g3sjOzlZaWpq6du2q2NjY0g8IklmzXLrxxmhZf2KuvO0ul/VH+PHHbl1/vWOZNuKEajtBaKGdwA7aCeygncCOSG4nmZmZqlWrlq0Q5NhwuFWrVmnv3r268MIL87a53W4tXrxYr732mrKyshR92hiv+Ph4xcfHFzpXbGxsQP6QA3VeX7jd0oMPSkVFVmNccrmkESNi1KePGBoXZKHUThC6aCewg3YCO2gnsCMS24k3n9exENS5c2etXbu2wLZBgwapWbNmeuSRRwoFoEi3ZIm0c2fxrxsj7dhh7dexY9DKAgAAAMKOYyGoSpUqatGiRYFtlSpVUs2aNQtth5Rhc7qP3f0AAACASOX4EtmwJynJv/sBAAAAkcrxJbLzW7hwodMlhKz27aWUFCk9veh5QS6X9Xr79sGvDQAAAAgn9ASFiehoacIE67HLVfA1z/Px41kUAQAAACgNISiM9O4tTZ8u1a9fcHtKirW9d29n6gIAAADCCSEozPTuLW3dKo0aZT1v0ULasoUABAAAANhFCApD0dFSjx7W44MHGQIHAAAAeIMQFKYaNrTud+2STp50thYAAAAgnBCCwlTdulJCgpSbW/JFVAEAAAAURAgKUy6X1KCB9XjbNmdrAQAAAMIJISiMNWpk3ROCAAAAAPsIQWHMMy9o61ZHywAAAADCCiEojNETBAAAAHiPEBTG6AkCAAAAvEcICmOeEERPEAAAAGAfISiMeYbD7dghud2OlgIAAACEDUJQGEtKkmJipJwc66KpAAAAAEpHCApj0dGnrhXEvCAAAADAHkJQmGNeEAAAAOAdQlCY88wLoicIAAAAsIcQFOboCQIAAAC8QwgKc/QEAQAAAN4hBIU5eoIAAAAA7xCCwpynJ2j7dik319FSAAAAgLBACApz9etLUVFSVpa0Z4/T1QAAAAChjxAU5mJjpZQU6zFD4gAAAIDSEYLKAc+8IBZHAAAAAEpHCCoHWBwBAAAAsI8QVA6wTDYAAABgHyGoHKAnCAAAALCPEFQO0BMEAAAA2EcIKgfy9wQZ42wtAAAAQKgjBJUDDRpY98eOSfv2OVsLAAAAEOoIQeVAfLyUlGQ9Zl4QAAAAUDJCUDnBvCAAAADAHkJQOcEKcQAAAIA9hKBywtMTRAgCAAAASkYIKic8PUEMhwMAAABKRggqJ+gJAgAAAOwhBJUT9AQBAAAA9hCCygnPtYIyM6WDBx0tBQAAAAhphKByolIlqXZt6zG9QQAAAEDxCEHlCMtkAwAAAKUjBJUjXDAVAAAAKB0hqByhJwgAAAAoHSGoHKEnCAAAACgdIagcoScIAAAAKB0hqByhJwgAAAAoHSGoHPH0BP3xh3T4sLO1AAAAAKGKEFSOJCZK1atbjxkSBwAAABSNEFTOMC8IAAAAKBkhqJzxzAsiBAEAAABFIwSVM56eIBZHAAAAAIpGCCpnGA4HAAAAlIwQVM6wTDYAAABQMkJQOUNPEAAAAFAyQlA54+kJ2rNHOn7c0VIAAACAkEQIKmeqV5cqV7Yeb9/ubC0AAABAKCIElTMuF/OCAAAAgJIQgsoh5gUBAAAAxSMElUP0BAEAAADF8zoEvfvuu/rvf/+b9/zhhx9WtWrVdOmll2obXQ8hgZ4gAAAAoHheh6BnnnlGFSpUkCQtW7ZMr7/+up5//nnVqlVLDzzwgN8LhPfoCQIAAACKF+PtATt27FDTpk0lSbNnz1afPn10991367LLLlPHjh39XR98QE8QAAAAUDyve4IqV66s/fv3S5K+/PJLde3aVZKUkJCg41yYJiR4eoJ27ZJOnnS0FAAAACDkeN0T1LVrV91555264IIL9Msvv6h79+6SpJ9++kmNPL++4ajataUKFayLpe7YITVp4nRFAAAAQOjwuifo9ddfV7t27fT7779rxowZqlmzpiRp1apVuummm/xeILznckkNGliPGRIHAAAAFOR1T1C1atX02muvFdo+ZswYvxQE/2jUSNq4kcURAAAAgNN53RM0d+5cLV26NO/566+/rvPPP1/9+/fXgQMH/FocfMfiCAAAAEDRvA5BDz30kDIzMyVJa9eu1YMPPqju3btry5YtGj58uN8LhG9YJhsAAAAomtfD4bZs2aJzzz1XkjRjxgxdc801euaZZ/T999/nLZIA59ETBAAAABTN656guLg4HTt2TJL01VdfqVu3bpKkGjVq5PUQwXn0BAEAAABF8zoEXX755Ro+fLiefPJJrVixQj169JAk/fLLL0pJSfHqXBMnTlTLli2VmJioxMREtWvXTnPmzPG2JBTB0xO0c6eUk+NsLQAAAEAo8ToEvfbaa4qJidH06dM1ceJE1a9fX5I0Z84cXXXVVV6dKyUlRc8++6xWrVql7777Tn/605/Us2dP/fTTT96WhdMkJUmxsZLbLaWnO10NAAAAEDq8nhPUoEEDffbZZ4W2v/zyy16/+bXXXlvg+dNPP62JEydq+fLlat68udfnwylRUda1gn791ZoX5OkZAgAAACKd1yFIktxut2bPnq3169dLkpo3b67rrrtO0dHRPhfidrs1bdo0HT16VO3atStyn6ysLGVlZeU998xBys7OVnZ2ts/vfTrPufx5Tic0bBitX3+N0ubNOWrXzjhdTrlTXtoJAot2AjtoJ7CDdgI7IrmdePOZXcYYr34db968Wd27d1d6errOPvtsSdLGjRuVmpqq//73v2rSpIlXxa5du1bt2rXTiRMnVLlyZU2ZMqXYVeZGjx5d5EVZp0yZoooVK3r1vpHg1VfP17x5DXXTTevVr98vTpcDAAAABMyxY8fUv39/HTp0SImJiSXu63UI6t69u4wx+vDDD1WjRg1J0v79+3XzzTcrKipK//3vf70q9uTJk9q+fbsOHTqk6dOn6+2339aiRYvyluHOr6ieoNTUVO3bt6/UD+qN7OxspaWlqWvXroqNjfXbeYPtqaeiNHZstAYNytWkSW6nyyl3yks7QWDRTmAH7QR20E5gRyS3k8zMTNWqVctWCPJ6ONyiRYu0fPnyvAAkSTVr1tSzzz6ryy67zOti4+Li1LRpU0lS69attXLlSk2YMEGTJk0qtG98fLzi4+MLbY+NjQ3IH3Kgzhssnk657dujFBvr9RoYsCnc2wmCg3YCO2gnsIN2AjsisZ1483m9/mUcHx+vw4cPF9p+5MgRxcXFeXu6QnJzcwv09sB3XDAVAAAAKMzrEHTNNdfo7rvv1rfffitjjIwxWr58ue69915dd911Xp1r5MiRWrx4sbZu3aq1a9dq5MiRWrhwoQYMGOBtWSiC54Kp27dLubmOlgIAAACEDK+Hw73yyisaOHCg2rVrl9fllJOTo+uuu07jx4/36lx79+7VrbfeqoyMDFWtWlUtW7bUF198oa5du3pbFopQv74UHS2dPCnt3i0lJztdEQAAAOA8r0NQtWrV9O9//1ubN2/OWyL7nHPOyZvX441//etfXh8D+2JipJQUazjc1q2EIAAAAEDy8TpBktS0adMCwefHH39UmzZtdPLkSb8UBv9o2NAKQdu2SZde6nQ1AAAAgPP8tmSYMUZuN8swhxrPvKCtW52sAgAAAAgdrJtczrFCHAAAAFAQIaicoycIAAAAKMj2nKDMzMwSXy/q2kFwHj1BAAAAQEG2Q1C1atXkcrmKfd0YU+LrcIanJ2jbNskYiT8iAAAARDrbIWjBggWBrAMBkppqBZ/jx6Xff5fq1HG6IgAAAMBZtkNQhw4dAlkHAiQuTkpKknbtsnqDCEEAAACIdCyMEAFYHAEAAAA4hRAUAVgcAQAAADiFEBQB6AkCAAAATiEERQB6ggAAAIBTCEERgJ4gAAAA4BTbq8N5XH/99UVeD8jlcikhIUFNmzZV//79dfbZZ/ulQJRd/p4grhUEAACASOd1T1DVqlU1f/58ff/993K5XHK5XFq9erXmz5+vnJwcTZ06Va1atdLXX38diHrhA08IOnxYOnDA2VoAAAAAp3kdgurVq6f+/fvrt99+04wZMzRjxgz9+uuvuvnmm9WkSROtX79eAwcO1COPPBKIeuGDChVOXR+IeUEAAACIdF6HoH/9618aNmyYoqJOHRoVFaW//vWveuutt+RyuTRkyBCtW7fOr4WibJgXBAAAAFi8DkE5OTnasGFDoe0bNmyQ2+2WJCUkJBQ5bwjOYYU4AAAAwOL1wgi33HKL7rjjDj366KO66KKLJEkrV67UM888o1tvvVWStGjRIjVv3ty/laJM6AkCAAAALF6HoJdffll169bV888/rz179kiS6tatqwceeCBvHlC3bt101VVX+bdSlAk9QQAAAIDF6xAUHR2txx57TI899pgyMzMlSYmJiQX2adCggX+qg994QhA9QQAAAIh0Xoeg/E4PPwhdnuFw9AQBAAAg0nm9MMKePXt0yy23KDk5WTExMYqOji5wQ2jy9AQdOCD9rwMPAAAAiEhe9wTddttt2r59u0aNGqWkpCRWgQsTVapINWpIf/xh9Qadd57TFQEAAADO8DoELV26VEuWLNH5558fgHIQSA0aWCHovfekHj2k9u0lOu8AAAAQabweDpeamipjTCBqQQDNnCl5Lu/0wgtSp07WPKGZMx0tCwAAAAg6r0PQ+PHj9be//U1bWWYsbMycKfXtK504UXB7erq1nSAEAACASOL1cLh+/frp2LFjatKkiSpWrKjY2NgCr//xxx9+Kw5l53ZLQ4dKRXXeGSO5XNKwYVLPngyNAwAAQGTwOgSNHz8+AGUgUJYskXbuLP51Y6QdO6z9OnYMWlkAAACAY7wOQQMHDgxEHQiQjAz/7gcAAACEO1shKDMzM+/CqJmlXGSGC6iGlqQk/+4HAAAAhDtbIah69erKyMhQnTp1VK1atSKvDWSMkcvlktvt9nuR8F379lJKirUIQlHzglwu6/X27YNfGwAAAOAEWyFo/vz5qlGjhiRpwYIFAS0I/hUdLU2YYK0C53IVHYTGj2dRBAAAAEQOWyGoQ4cORT5GeOjdW5o+3Vol7vRFEl5/3XodAAAAiBReL4wgSQcPHtSKFSu0d+9e5ebmFnjt1ltv9Uth8K/eva1lsJcssRZBePFFadUqac0apysDAAAAgsvrEPTpp59qwIABOnLkiBITEwvMD3K5XISgEBYdfWoZ7JQU6YorpHfekf7+dyk11dHSAAAAgKCJ8vaABx98ULfffruOHDmigwcP6sCBA3k3LpQaPtq3lzp0kLKzpeeec7oaAAAAIHi8DkHp6em6//77VbFixUDUgyB6/HHr/u23pV27nK0FAAAACBavQ9CVV16p7777LhC1IMg6dZIuvVTKypL+8Q+nqwEAAACCw+s5QT169NBDDz2kn3/+Weedd55iY2MLvH7dddf5rTgElstl9QZddZU0aZI0cqRUp47TVQEAAACB5XUIuuuuuyRJY8eOLfQaF0sNP926SRddJK1caa0Yx/wgAAAAlHdeD4fLzc0t9kYACj+e3iDJumbQvn3O1gMAAAAEmtchCOVPjx7SBRdIR49K48c7XQ0AAAAQWLaGw73yyiu6++67lZCQoFdeeaXEfe+//36/FIbgcbmkUaOsC6q+8or04INS9epOVwUAAAAEhq0Q9PLLL2vAgAFKSEjQyy+/XOx+LpeLEBSmevaUWrSQ1q2zgtATTzhdEQAAABAYtkLQli1binyM8iMqyuoN6tfPGhL3wANSYqLTVQEAAAD+x5wg5OnTR2rWTDp4UHrtNaerAQAAAALD6yWyJWnnzp36z3/+o+3bt+vkyZMFXnvppZf8UhiCLzpa+vvfpZtvll56Sbr/fqlyZaerAgAAAPzL6xA0b948XXfddTrjjDO0YcMGtWjRQlu3bpUxRhdeeGEgakQQ9esnjR4tbd4sTZwoPfSQ0xUBAAAA/uX1cLiRI0dqxIgRWrt2rRISEjRjxgzt2LFDHTp00J///OdA1IggiomRHnvMevzCC9KxY87WAwAAAPib1yFo/fr1uvXWWyVJMTExOn78uCpXrqyxY8fqueee83uBCL4BA6RGjaS9e6W33nK6GgAAAMC/vA5BlSpVypsHlJSUpF9//TXvtX379vmvMjgmNlZ69FHr8fPPSydOOFsPAAAA4E9ezwm65JJLtHTpUp1zzjnq3r27HnzwQa1du1YzZ87UJZdcEoga4YCBA6Unn5R27JD++U/pvPOkjAwpKUlq395aRAEAAAAIR16HoJdeeklHjhyRJI0ZM0ZHjhzR1KlTdeaZZ7IyXDkSFyf97W/S4MHWNYPc7lOvpaRIEyZIvXs7Vx8AAADgK69CkNvt1s6dO9WyZUtJ1tC4N998MyCFwXk1alj3+QOQJKWnS337StOnE4QAAAAQfryaExQdHa1u3brpwIEDgaoHIcLtLn55bGOs+2HDCgckAAAAINR5vTBCixYt9NtvvwWiFoSQJUuknTuLf90Ya77QkiXBqwkAAADwB69D0FNPPaURI0bos88+U0ZGhjIzMwvcUD5kZPh3PwAAACBU2J4TNHbsWD344IPq3r27JOm6666Ty+XKe90YI5fLJTfjo8qFpCT/7gcAAACECtshaMyYMbr33nu1YMGCQNaDENG+vbUKXHr6qTlA+blc1uvt2we/NgAAAKAsbIcg879fwh06dAhYMQgd0dHWMth9+1qBJ38Q8nQAjh/P9YIAAAAQfryaE5R/+BvKv969rWWw69cvuD0lheWxAQAAEL68uk7QWWedVWoQ+uOPP8pUEEJL795Sz57SLbdIH31kPZ4xgx4gAAAAhC+vQtCYMWNUtWrVQNWCEBUdLXXvboWgP/4gAAEAACC8eRWCbrzxRtWpUydQtSCEtWhh3a9bZ80PYmQkAAAAwpXtOUHMB4pszZpZPUAHDnBtIAAAAIQ32yHIFLVOMiJGQoJ05pnW43XrnK0FAAAAKAvbISg3N5ehcBHOMyRu7Vpn6wAAAADKwqslshHZ8s8LAgAAAMKVoyFo3Lhxuuiii1SlShXVqVNHvXr10saNG50sCSU47zzrnhAEAACAcOZoCFq0aJEGDx6s5cuXKy0tTdnZ2erWrZuOHj3qZFkohqcn6KefpNxcZ2sBAAAAfOXVEtn+Nnfu3ALPJ0+erDp16mjVqlW64oorHKoKxWnSRIqPl44fl377TWra1OmKAAAAAO85GoJOd+jQIUlSjRo1inw9KytLWVlZec8zMzMlSdnZ2crOzvZbHZ5z+fOc5cU558RozRqX1qzJUcOGkb1iIO0EdtBOYAftBHbQTmBHJLcTbz6zy4TI2te5ubm67rrrdPDgQS1durTIfUaPHq0xY8YU2j5lyhRVrFgx0CVC0oQJF2jBggbq33+9brjhF6fLAQAAACRJx44dU//+/XXo0CElJiaWuG/IhKC//OUvmjNnjpYuXaqUlJQi9ymqJyg1NVX79u0r9YN6Izs7W2lpaeratatiY2P9dt7y4MUXozRyZLT69s3VlClup8txFO0EdtBOYAftBHbQTmBHJLeTzMxM1apVy1YIConhcEOGDNFnn32mxYsXFxuAJCk+Pl7x8fGFtsfGxgbkDzlQ5w1nrVpZ9z//HKXYWFZYl2gnsId2AjtoJ7CDdgI7IrGdePN5Hf0Va4zRkCFDNGvWLM2fP1+NGzd2shzY4Fkh7pdfpHydcgAAAEDYcDQEDR48WB988IGmTJmiKlWqaPfu3dq9e7eOHz/uZFkoQUqKVLWqlJNjBSEAAAAg3DgagiZOnKhDhw6pY8eOSkpKyrtNnTrVybJQApfrVG/Q2rXO1gIAAAD4wtE5QSGyJgO81KKF9PXX0rp1TlcCAAAAeI+Z7fDaeedZ94QgAAAAhCNCELzmGQ5HCAIAAEA4IgTBa82bW/dbtkiHDztbCwAAAOAtQhC8VquWVK+e9fjnn52tBQAAAPAWIQg+YV4QAAAAwhUhCD5hmWwAAACEK0IQfMLiCAAAAAhXhCD4hBAEAACAcEUIgk88K8Tt2SP9/ruztQAAAADeIATBJ5UqSWecYT2mNwgAAADhhBAEn4XSkDi3W1q4UProI+ve7Xa6IgAAAIQqQhB8FirLZM+cKTVqJHXqJPXvb903amRtBwAAAE5HCILPQqEnaOZMqW9faefOgtvT063tBCEAAACcjhAEn+UPQcYE//3dbmno0KLf27Nt2DCGxgEAAKAgQhB8dtZZUkyMlJkp7dgR/PdfsqRwD1B+xlh1LVkSvJoAAAAQ+ghB8FlcnNSsmfXYiSFxdoNXRkZg6wAAAEB4IQShTJyaFzRnjvTYY/b2TUoKbC0AAAAIL4QglIknBK1dG5z327BB6t7duu3YIUWV0IJdLik1VWrfPji1AQAAIDwQglAm/u4JKu56PwcOSA88YC3LPWeOFBsrPfigNHmyFXZcroLn8TwfP16KjvZPbQAAACgfYpwuAOHNc62g9eulnBxroQRfzZxprfaWf7GDlBSr12fGDGn/fmvbtddKL74onXmm9bxSpcLH1a8vTZgg9e7tez0AAAAon+gJQpk0aiRVrChlZUmbN/t+nuKu97Nzp/TWW1YAOvdc6YsvpP/851QAkqygs3WrNH++VKWKtW3GDAIQAAAAikYIQplERUnNm1uPfR0SV9L1fjyqVZO+/17q1q3o16OjpU6dpLZty1YLAAAAyj9CEMqsrPOCSrvejyQdPCgtW1b6uVq2tO5/+MG3WgAAAFD+EYJQZp55Qb6GILvX8bGzX6tW1j0hCAAAAMUhBKHMyrpMtt3r+NjZL38IKml4HQAAACIXIQhl5glBmzdLx497f3z79tYqcKcvc+3hzfV+zjnHWqHu4EHrOkIAAADA6QhBKLN69aSaNaXcXOtipt6KjraWsy6q58bb6/3Ex1tBSGJIHAAAAIpGCEKZuVxlXxyhd2+pSZPC21NSpOnTvVvumnlBAAAAKAkXS4VftGghLVrk+7yg1aulX3+1hrJNm2YNq0tKsobA2ekByq9VK+mDD6Qff/StFgAAAJRvhCD4RVl7gt56y7rv00fq1atstdATBAAAgJIwHA5+UZZlso8ckT780Hp8991lr8VzraBNm6SjR8t+PgAAAJQvhCD4RfPm1v2OHdKhQ94dO3WqdPiw1LSp1KlT2WupW9e6GeN7zxQAAADKL0IQ/KJaNWsRA8n74OEZCnf33cUvk+0thsQBAACgOIQg+I0v84LWrJFWrJBiY6WBA/1XCyEIAAAAxSEEwW98mRfk6QW6/nqpTh3/1UIIAgAAQHEIQfAbT0+Q3WWyjx61lrKWpHvu8W8tnhD044/WRVwBAAAAD0IQ/Cb/cDhjSt//449PLYjQsaN/azn7bCkuzjr/tm3+PTcAAADCGyEIfnPOOVJUlLR/v7RnT+n7e4bC3XWXdZw/xcaeWrGOIXEAAADIjxAEv6lQwerVkUqfF5R/QYTbbgtMPZ7rBRGCAAAAkB8hCH5ld17QP/9p3ft7QYT8WBwBAAAARSEEwa/sLJOdf0GEu+8OXC2EIAAAABSFEAS/srNM9tSpUmam1KSJ1KlT4GrxhKDffrPeDwAAAJAIQfAzT0/QTz8VvzR1IBdEyK9mTal+feux3WW7AQAAUP4RguBXTZtaS1MfPSpt3Vr49R9+kL79VoqJCdyCCPkxJA4AAACnIwTBr2JirKWypaKHxHl6ga6/XqpbN/D1EIIAAABwOkIQ/K64eUHBWhAhP08I+vHH4LwfAAAAQh8hCH5X3DLZn3xiLVBwxhnSn/4UnFo81wpau7b4OUoAAACILIQg+F1xy2QHa0GE/M48U0pIsHqhfv01OO8JAACA0EYIgt95QtCGDdLJk9bjH3+Uli+35gwNGhS8WmJiTtXDvCAAAABIhCAEQIMGUpUqUk6OtGmTtc3TC9SrV3AWRMiPxREAAACQHyEIfudyFZwXdOyY9P771vNgLYiQHyEIAAAA+cU4XQDKpxYtpGXLrHlBJ06cWhChc+fg10IIAgAAQH6EIATEueda93PnWj1BUnAXRMjPs0Lc9u3SgQNS9erBrwEAAAChg+Fw8LuZM6VnnrEer1olrV9vPa5d25l6qlWTGja0HnO9IAAAABCC4FczZ0p9+0q//174tbvusl53gqc3iBAEAAAAQhD8xu2Whg6VjCl+n2HDrP2CjXlBAAAA8CAEwW+WLJF27iz+dWOkHTus/YKNEAQAAAAPQhD8JiPDv/v5kycErVtnXb8IAAAAkYsQBL9JSvLvfv7UpIlUqZK1XLfnAq4AAACITIQg+E379lJKinWx1KK4XFJqqrVfsEVFSeedZz1mSBwAAEBkIwTBb6KjpQkTrMenByHP8/Hjrf2cwLwgAAAASIQg+Fnv3tL06VL9+gW3p6RY23v3dqYuiRAEAAAAS4zTBaD86d1b6tnTWgUuI8OaA9S+vXM9QB6eawURggAAACIbIQgBER0tdezodBUFeULQrl3Svn1SrVrO1gMAAABnMBwOEaNKFemMM6zHP/7obC0AAABwDiEIEYV5QQAAACAEIaIQggAAAEAIQkQhBAEAAIAQhIjiCUE//yxlZztbCwAAAJzhaAhavHixrr32WiUnJ8vlcmn27NlOloMI0KiRtUDCyZPShg1OVwMAAAAnOBqCjh49qlatWun11193sgxEEJeL6wUBAABEOkevE3T11Vfr6quvtr1/VlaWsrKy8p5nZmZKkrKzs5Xtx7FNnnP585wIHeedF6Wvv47W6tVu9euX6/N5aCewg3YCO2gnsIN2AjsiuZ1485nD6mKp48aN05gxYwpt//LLL1WxYkW/v19aWprfzwnnuVwNJZ2v+fP36/PPl5X5fLQT2EE7gR20E9hBO4EdkdhOjh07ZntflzHGBLAW21wul2bNmqVevXoVu09RPUGpqanat2+fEhMT/VZLdna20tLS1LVrV8XGxvrtvAgNK1a4dPnlMapb12jHjhyfz0M7gR20E9hBO4EdtBPYEcntJDMzU7Vq1dKhQ4dKzQZh1RMUHx+v+Pj4QttjY2MD8occqPPCWeefb80N2rPHpT/+iFXdumU7H+0EdtBOYAftBHbQTmBHJLYTbz4vS2Qj4lSqJJ15pvWYxREAAAAiDyEIEYmLpgIAAEQuR0PQkSNHtGbNGq1Zs0aStGXLFq1Zs0bbt293sixEAEIQAABA5HJ0TtB3332nTp065T0fPny4JGngwIGaPHmyQ1UhEnCtIAAAgMjlaAjq2LGjQmRxOkQYT0/Qhg1SVpZUxHobAAAAKKeYE4SIlJoqVasm5eRI69c7XQ0AAACCiRCEiORyMS8IAAAgUhGCELEIQQAAAJGJEISIRQgCAACITIQgRKz8IYj1OQAAACIHIQgRq3lza27Q/v3SG29ICxdKbrfTVQEAACDQCEGIWJ9/LkVHW4+HDJE6dZIaNZJmzgzce7rdVtj66CNCFwAAgFMIQYhIM2dKfftaS2Tnl55ubQ9EEJo50wpZnTpJ/fsHJ3QBAACgMEIQIo7bLQ0dWvQ8IM+2YcP820vjCV07dxbcHsjQBQAAgKIRghBxliwpHEbyM0bascPazx+cCF0AAAAoHiEIEScjw7/7lSbYoQsAAAAlIwQh4iQl2duvWjX/vF96ur39/BW6AAAAUDJCECJO+/ZSSoq1PHZJBg2S/vnPwosn2HXihPTWW9Ijj9jb3244AwAAQNkQghBxoqOlCROsx6cHIc/zunWlPXuku++2Lqr6+ecF5/S43dKiRS4tXlxfixa5Cszn+eMP6emnpYYNpXvusXqCSgpcLpeUmmqFMwAAAAQeIQgRqXdvafp0qX79gttTUqQZM6Tt26Xx46UaNaSff5Z69JC6dpVWrz611HXXrjF66aU26to1Ro0aSZMmWQscNGgg/f3v0t691uOXX5bef98KO8WFofHjT12zCAAAAIEV43QBgFN695Z69rQWJMjIsIajtW9/KowMHSrdeqv0zDPSK69I8+ZJF15Y9Ll27pTuvffU81atpIcekm64QYqNtbZVqGCd8/RFEp54wqoFAAAAwUFPECJadLTUsaN0003W/em9MdWrS//4h7Rxo9SvX+nni4+3hs6tXi0NGHAqAElW0Nm6VVqwQJoyRerVy9o+ZYqUleWfzwMAAIDSEYIAGxo1KtjTU5ysLKvHp7hhb/lD1+TJUr160i+/SM8/78diAQAAUCJCEGCTv68vVLWqNV9IshZS2LzZt7oAAADgHUIQYJPdJay9Weq6Xz+pWzerB+m++wquQAcAAIDAIAQBNpV2fSFflrp2uaTXX7fmEqWlSVOn+qdWAAAAFI8QBNhk5/pCvix13bSp9Nhj1uMHHpAOHixLlQAAACgNIQjwQknXF5o+3felrh9+WDrrLGn3busaQwAAAAgcQhDgJc9S12lpORo+/DulpeVoy5ayXesnPl6aONF6/MYb0sqVfikVAAAARSAEAT6IjpY6dDC64op0dehgvB4CV5Q//Um6+WZrcYR77pFycsp+TgAAABRGCAJCyAsvSNWqWRdbfeMNp6sBAAAonwhBQAipW1d69lnr8d//LqWnO1sPAABAeUQIAkLMXXdJl1wiHT5srRYHAAAA/yIEASEmKkp6801r3tG0adKcOU5XBAAAUL4QgoAQ1KqVNHSo9fi++6QvvpA++khauFByu+2dw+229vf2OAAAgPKOEASEqNGjpZo1reW4r7pK6t9f6tRJatRImjmz5GNnzrT269TJu+MAAAAiASEICFFpadL+/YW3p6dLffsWH2hmzrRe37nTu+MAAAAiBSEICEFu96nhcKczxrofOrTwtYQ8x3n2Keq4YcMYGgcAACJbjNMFAChsyZLCPTn5GWO9Hh8vJSZKlStbN8/2ko7bscM6f8eOfi8bAAAgLBCCgBCUkWFvv9xc6eBB6xaI8wMAAJRHhCAgBCUl2dvvk0+kli2lI0es2zffSI8+WvpxMfyXDwAAIhg/hYAQ1L69lJJiLWZQ1Pwel8t6vXdv63pCHpdfLr3xRvHHeQwYIC1YIP3tb1KDBgVfc7ut4XIZGVYYa9++4HsAAACEOxZGAEJQdLQ0YYL12OUq+Jrn+fjxhcOJneOaNZOys6WJE6WmTaW77pJ++816jaW1AQBAJCAEASGqd29p+nSpfv2C21NSrO29e3t/3IwZ0s8/W71Af/qTFYbefls66yxroYRwWVqbC8ECAICyYDgcEMJ695Z69vR+eFppx3XsaN2++UZ68klp7lxp0aKiz2WM1Ys0bJh1TqeHxs2caS0Dnj+spaRYPWDFBUMAAID8CEFAiIuO9m05azvHXXqpNGeO9Oab0l/+Uvx+obK0tudCsKfPd/L0VpXUQwYAAODBcDgAqlrV3n5OLq0dSReCZbgfAACBRQgCYHtJbrv7BYKdC8h6eqvCGYtTAAAQeIQgAHlLcp++olx+MTFSQkLwajqd3V6ocL4QrGe4XzgsTgEAQDgjBAEocWltj5wc6zpEY8daj4OtVi17+znZW1UWkTTcDwAApxGCAEgqfmnt1FRp8mSpXz/rB/gTT1hhaNOm4NW2c6c0apS9fTdsCGwtgRIpw/0AAAgFhCAAeXr3lrZuta4jNGWKdb9lizRwoPTxx9KHH1qLKHz7rXT++dKkSad6KQI1mX/ePOmCC6z3rFjR2lbchWAla5W7oUOd6a0qi0gY7lcWLBYBAPAnQhCAAjxLa990k3Wf/7pA/ftLa9daF1o9dky6917p2mulf/3L/5P5c3OlZ56RunWT9u2zgtDatdYFX4u7gOxTT1nPX3lFuuYa6eBB398/2GrUsLdfuA73KwsWiwAA+BvXCQLgldRUKS3NmkM0cqT03/9at9PZvXaP2134oq6ZmdKtt0qffWbtc8cd0quvShUqSGecUfKFYJs1s4794gupXTvp00+lpk39/z34088/SyNGlL5faqr1WSMJ14YCAAQCIQiA16KipAcesHqELrpIys4uvI8x1jC1YcOs0JK/R8lj5kxr6Fr+uTB16lj3e/dK8fHS669bISi/ki4E26eP1LixdN111vygiy+2fih36lR04CqqrtP5elxpjJHeftv6Do4flxITrQDochW9QML99/vnfcNFaYtFlNa+AAAoDsPhAPjswIGiA5CHZzL/hx8WnsNR3HLQe/datzp1pGXLCgcgOy68UFq5UmrbVvrjD2tI3b33+jakKlBDsQ4etBabuPtuKwB17Spt3Fj0cL+4OOv+2WeldevK9r7hhMUiAACBQggC4DO7k/QHDrR6OS6+WLrnHum116xQUtS/8HvExkotW/peW1KSNYH+xhutRRImTfL++jtlvW6P2y0tWuTS4sX1tWiRKy8ILltmLSwxbZp1/aXnn5fmzpXq1St6cYqMDKvHbf9+qUsXKyxFAhaLAAAECsPhAPjM7iT9uDhrIYUVK6ybHenp1r/wFzfszY4KFaT335c+/9waZnY6z5CqoUOtZb+PHZMOH5aOHLF6au6+2/ehWKeG+sVIaqOXXrIWcLjiCmnqVCsgnXGGtdpZ27YFjy1quN/cudbwwx9+kDp3lhYvto4vj4yxAuxzz9nbf8+eU38mpwvUUEZ/yh+WK1VyqVOn0KsRAMobeoIA+Kx9e+uHfXEXWHW5rMn8hw9b83M+/thaTOH88+2d3x//wr90adEByMMYq6enbl1rLlHLltKll0rdu1s9LyUdt2OHdf2ijRut1ew8iutB2rnT6t1xu63V91avLhyAilOjhrUgxbnnWgHxT3+Stm+3d2woKWmpa2OkL7+02pUn8NnxwANS69bSBx9IJ0+e2h4Oq8p5auzaNUYvvdRGXbvGhFyNAFAeEYIA+Cw62lolTir+2j3jx1s9QWefbc2BeeYZ6eWX7Z3fH8tBexOkEhKk2rWtHpbUVHvHjBtnrUhXs6Y19+jRR4vvQfKoUUN67z1riKA3ate2rpt01lnStm1WUNi1y7tzOKm4UDJjhrXC4CWXSFdeKX39tbUoxpAh1jBGl6vo9uVyWftXqGAFyltusYLss89K777r+1DGYF2TqKzDLQEAviMEASiT3r2t1deKu3ZPUcsX2+1B8sdy0HaD1FdfWQsU7N0r/fqrFVLsaNHCCk8HD1o9NePGldyDJFmLNSxdau/8p6tXzwpCjRtbdXbubNUc6krqHevb17qu04oVVqB54AHrIr2vvmoFypLa19y5Vo/c009bf9a7dlm9jYMGFT+UUbKGMhYVboLVe1Tayncl1QgAKDtCEIAyK2oy/5YtxV+/xW4Pkj/mRdgNXKfPwbF73Jo11nC777+X3nxT6tDBXl1lGeqXkiLNn2+9/4YN1mIJe/f61nsRjF6Pkn7we7hc0oMPWu3mpZcKhtfS2lfNmlYP3NatVnht0qTk9/IMZVy8uOD2YPbMsPIdADiLhREA+EVJ1+4piqcH6fTrBKWkWAHIXxfA9ASuvn0LX3+npMDlzXHR0dIFF1i3s8+2eg9KU9ahfo0aWT1CHTpIa9da31v+5cpTUqz6S/oei7pOk53jJO8WHCjtB79kfb/XXGPNzSqKnfYVF2cNiYuOlgYMKHlfyVqWvH59KTnZ6mFLSwveNYnS0+3tx8p3ABAY9AQBcIy3PUhleR9vh+z5elwwh/qdeab0yCPW49Ov1xTI5b/tDBn74w9p1izrAq+33GLv8/jrB39ysr393G5rcYnly6XZs6WjR4vf107PjJ1etS1brMU0hg2zV2NxoRAAUDb0BAFwlLc9SL7q3dv6V3xvl0v29jhfe5584XZLL7xQ9Gue9737bmvFtOjoU4sJ5OZKf/lLyb0eQ4cW3evhCU+nH5ueLvXpI113nRWsVq8ueUhaUfyxEIZ0Koimpxddg8tlBdvFi63ltXftkj79VJo8ufRz33efdNtt1uqBzZuf+jMtqVete3crZP3rX9bcs/x1lPYdPfigNc/syiuLD9You3BYSh2AfxGCAEQMXwNXqA71szPMbP9+azlub3iWDa9USapVS6peXapWzbrNn1/yZP7//OfUtnPOsXqJOnSwFjvIyCg+lKSk+Kd3TLIXRCdMsBaXaNzYel6jhr0QtH691fv2yCNWj1737tb389xzxQfDypWta095dO0q3Xmntb/nz+b0Go2xFtxYs0a6+mrr+lLjxlnLt3v4+sPdl+OC+V7BVpZhoQDCmAljhw4dMpLMoUOH/HrekydPmtmzZ5uTJ0/69bwoX2gnKE1OjjFpadlm+PCVJi0t2+Tk+Pf8U6YYY/1cLvl2zjnGXHGFMe3bG3P55cacdZa943y9/f3vxuzaVbDWGTOMcbmsW/59PdtmzPDvd+N5z5SUgu+Xmlr0e+XkWPueXl/+OpOSjJkwwZirrzYmIcG776R+fWMef9yYLVvs1/j778YMH25MfPyp16691pgffyz6uJSU0r9HX44L5nvl//NYsMBq4wsWGL//t5O/xqL+zAPZLsvy2fj/DuyI5HbiTTYgBBUhkhsP7KOdwI5AtpMFC+z9AF+wwLfjPvzQmO++MyYtzZhPPjHmzjvtHTdlStH1ehNK/MWbH5zeBLVjx4z5/HNjevWy95189VXJNZYUlrdvN+aOO4yJiir5PUr74e7LD35fQ0JZwkVZwpM3PMG3pO8zNbXkNuNtoCnrZ+P/OwUFKyyHm0huJ4SgMorkxgP7aCewI5DtxE7vRVE/4nw9ztfQdfp7h/KPFm+Dmt3euOKCoYeddrJhgzF9+5YehJKTjfnjD2Oys08d68sPfrvHnDxpzIkTxmRmGrN/vzE7d1q9Zr6Ei2D2zJS1PXsbaMr62XztWQ71/+aM8a3GYIXlcBTJv08IQWUUyY0H9tFOYEeg24mvw8x8Oc7X8BRuvPlB5o9gaIz9dmL3/Ty3uDhjqlUzpmZNe/ufeaYxrVoZ06KFMQ0aePde3t5q1TKmeXNjLr3UGmJ4ww3GVKrkW3jy5s8uN9eYVavs9+KddZYx991nzKRJxixbZsyRI94HmrL2OpX3IYm+DNEM9jDGcBLJv08IQWUUyY0H9tFOYEcw2omvw8x8Oc6JuT2hzF/B0G47sdvzVJ5vr7xiDUc8XUk/pnNyjFm82JgHHjCmYcOy1xATU/LrVasaM3Kk9X733GNM1672zvvFF0V/rnAakujLEFRvanRiGKMTx4XL3LFQ62kkBJURP25hB+0EdgSrnQTzf85OzO0JZf4Ihv7uCZo715h9+6z5RBs2WL0Ydo4bN8469quvjHn5ZXvHzJ5tzKFDVjDxtCc7x735pjHz5lnfz//9nzG33OJdCGnd2uqhefddY159tfggKhmTmFjwecWKxvTubUyNGiUH2Hr1jHnvPWMeesiYbt2s52UNUCXdXC5jmjQxpkcPYx580Ji33jKmdu3Sf/BnZ1vDEbdtM+aHH4yZP7/k3r9ADUn0JjzZCTN161pz7957z5gXXjDm4YeNufJKe9/l55+XvUanjitrCA3WsMlQHJJICCojftzCDtoJ7Civ7STU/vXPaWUNhnbbSTDngYXqnLPq1X0LGFWrGnPrrcbMmmXM0aOn/ty8DbATJ9p7vyuvNOaRR4wZM8aYe+8NTGjy3EpbNKO4W/36Vi/VnXca89RTVtiwE7p8DU8nTlgrJC5daswTTwT2O3G5rJUx+/e3AtS8eVZgDmavWjAXI/EcG8ygFopDEsMuBL322mumYcOGJj4+3rRt29Z8++23to4jBMFJtBPYQTuJHMEavhLMeWChOOcsO9uYrVuNmTrVWkK8eXN7P4rT0oqv1ZsA68s8MLufLT3dOm7iRGOGDjXmvPO8++EfG2tMnTolL0zhj1unTsYMHGgN9bv/fmNGjDCmSpWSj/E1qKWmGtOlixVmhg0z5q67Ave5ate2ekGXLjXm22+N+f57Y9asKbkHsLR/DPDmuLIM9QtmUPPHkMRA8SYbOH6x1KlTp2r48OF68803dfHFF2v8+PG68sortXHjRtWpU8fp8gAAsMXXi/F6y9eL8fpyXDDfy85FbsePl2JipIYNrdsNN0ht2kj9+xddR36//158rT172r+oa/v21udITy9YY/5aT7/4r93Plpxs3TztaOFC64LDpZk2zbpwb4UK1vnsHvfyy9bFfrdutW4rVkg//1z6cQsWlL7P6XJzrfv4eOszVqokrVtX+nHvvVfwvyu3W5ozp/Tv/5tvpB9/lFavlr7/3nq+e3fJ7/X771KXLrY/kiSrhh07rIsn16wpVaxo3Y4fL/li1p7jOnSQEhOlkyelPXvsHXPhhVKdOtZ3GRcnxcZKn31W9Pfh2Xb77dLmzdb+MTHWLSpK+tvfSj7uttukuXOlo0elw4et265d9upcsiQ4fyf6LAihrERt27Y1gwcPznvudrtNcnKyGTduXKnH0hMEJ9FOYAftBHb40k7CYZJ2MOac+WuFPm+UpYfMm88WqkMSBw825tlnraF+I0ca0727vePeestana8sNfr6/dtdVCQ52ZqX1aCB1aNWWg8Xt+JvpV0aIBDCpifo5MmTWrVqlUaOHJm3LSoqSl26dNGyZcsK7Z+VlaWsrKy855mZmZKk7OxsZWdn+60uz7n8eU6UP7QT2EE7gR2+tpPLLjv1ODf31L+2B+K4YL3XtddavRpLl7ryemYuv9woOloq6uu55BKpfv0Y7dolGeMq9LrLZVS/vnTJJTlFHu+La6+VPv7YpeHDo5Wefuo969c3evFFt6691hT5Xt5+Nkl68UWXbrwx+n89SKfey+UykqQXXnArN9cU+l59Oc7ud/nCCzkFesoWLXLp889L/0nZuHGOcnJMmT+bL99/7douSaXX+O67OerQ4VSNixa51LVr6ceNH+/W2WcbHTtm9QKtWePSCy8U052Yz7BhbrVoYRQbK23e7NKTT5Z+zN//7laTJtZnPHnSpWXLXPrww6hSj7vsslylpEg5OdZt2zZpzZrSj7v++ly1a2dUpYpR5crS1q0ujRpVep21a+coO9uUup8/efN3qKMhaN++fXK73apbt26B7XXr1tWGDRsK7T9u3DiNGTOm0PYvv/xSFStW9Ht9aWlpfj8nyh/aCeygncAO2klBiYnWMJwvvih5v5tvTtJzz10kyUjK/+PdyBhpwICV+uKLDL/WFh8vvfKK9PPPNXXgQIKqVz+hc8/dr+ho6fPPSz/e7meLj5cefjhJb799nvbvr5C3vWbN47rjjnWKj88o8v18Pc6X79LtlmrW7Kb9+xNOO+bUsbVqHVdmZlqB9/S1Rs+x3nz/vtZo97jU1DRlZVlDHytXltq1s3dc+/ZpeYHy/PPtHXPBBWkFQmizZjUlXV7U11TAVVd9o/PO25/3fO3amlqzpvTjLrzwG5111qnjzj3Xt+8yGI4dO2Z7X5cxJrgRLZ9du3apfv36+uabb9SuXbu87Q8//LAWLVqkb7/9tsD+RfUEpaamat++fUpMTPRbXdnZ2UpLS1PXrl0VGxvrt/OifKGdwA7aCeygnZTdrFmFewZSUqyegeuvd+ynjt+43dLChW6lpa1T164t1LFjdLHzlk4/rqiep5L48l3OmmX16khF9+p8/HHxx/pSoy98rTGYx/lyjNstNW1aeg/epk0Fe/B8Pa4s30mgZWZmqlatWjp06FCp2cDRnqBatWopOjpae/bsKbB9z549qlevXqH94+PjFR8fX2h7bGxsQP6nEajzonyhncAO2gnsoJ347oYbpD59Tl/gwKXoaMfXgPKL2Fipc2cpKytdnTu3st1OYmO9n+zvy3d5ww3WZPvCi2G4/rcYRvHH+lKjL3ytMZjH+XJMbKzVK1b8whsuTZggJSTE+uW4snwngebN35+O/s0QFxen1q1ba968eerVq5ckKTc3V/PmzdOQIUOcLA0AAISZYK3QFwl8+S69XWnPCb7WGMzjfD0mWCs5lqXOUOL4P48MHz5cAwcOVJs2bdS2bVuNHz9eR48e1aBBg5wuDQAAAF4IhyDqa43BPK4sIXTBghzNmbNGV199vjp1iglYwPO1zlDheAjq16+ffv/9dz3++OPavXu3zj//fM2dO7fQYgkAAAAAihcdLXXoYHT0aLo6dGhlu1cmnMOMrxwPQZI0ZMgQhr8BAAAACIrSFwcHAAAAgHKEEAQAAAAgohCCAAAAAEQUQhAAAACAiEIIAgAAABBRCEEAAAAAIgohCAAAAEBEIQQBAAAAiCiEIAAAAAARhRAEAAAAIKIQggAAAABEFEIQAAAAgIgS43QBZWGMkSRlZmb69bzZ2dk6duyYMjMzFRsb69dzo/ygncAO2gnsoJ3ADtoJ7IjkduLJBJ6MUJKwDkGHDx+WJKWmpjpcCQAAAIBQcPjwYVWtWrXEfVzGTlQKUbm5udq1a5eqVKkil8vlt/NmZmYqNTVVO3bsUGJiot/Oi/KFdgI7aCewg3YCO2gnsCOS24kxRocPH1ZycrKiokqe9RPWPUFRUVFKSUkJ2PkTExMjrvHAe7QT2EE7gR20E9hBO4EdkdpOSusB8mBhBAAAAAARhRAEAAAAIKIQgooQHx+vJ554QvHx8U6XghBGO4EdtBPYQTuBHbQT2EE7sSesF0YAAAAAAG/REwQAAAAgohCCAAAAAEQUQhAAAACAiEIIAgAAABBRCEGnef3119WoUSMlJCTo4osv1ooVK5wuCQ5avHixrr32WiUnJ8vlcmn27NkFXjfG6PHHH1dSUpIqVKigLl26aNOmTc4UC8eMGzdOF110kapUqaI6deqoV69e2rhxY4F9Tpw4ocGDB6tmzZqqXLmy+vTpoz179jhUMZwwceJEtWzZMu8Chu3atdOcOXPyXqeNoCjPPvusXC6Xhg0blreNtgJJGj16tFwuV4Fbs2bN8l6nnZSMEJTP1KlTNXz4cD3xxBP6/vvv1apVK1155ZXau3ev06XBIUePHlWrVq30+uuvF/n6888/r1deeUVvvvmmvv32W1WqVElXXnmlTpw4EeRK4aRFixZp8ODBWr58udLS0pSdna1u3brp6NGjefs88MAD+vTTTzVt2jQtWrRIu3btUu/evR2sGsGWkpKiZ599VqtWrdJ3332nP/3pT+rZs6d++uknSbQRFLZy5UpNmjRJLVu2LLCdtgKP5s2bKyMjI++2dOnSvNdoJ6UwyNO2bVszePDgvOdut9skJyebcePGOVgVQoUkM2vWrLznubm5pl69euYf//hH3raDBw+a+Ph489FHHzlQIULF3r17jSSzaNEiY4zVLmJjY820adPy9lm/fr2RZJYtW+ZUmQgB1atXN2+//TZtBIUcPnzYnHnmmSYtLc106NDBDB061BjD3yc45YknnjCtWrUq8jXaSenoCfqfkydPatWqVerSpUvetqioKHXp0kXLli1zsDKEqi1btmj37t0F2kzVqlV18cUX02Yi3KFDhyRJNWrUkCStWrVK2dnZBdpKs2bN1KBBA9pKhHK73fr444919OhRtWvXjjaCQgYPHqwePXoUaBMSf5+goE2bNik5OVlnnHGGBgwYoO3bt0uindgR43QBoWLfvn1yu92qW7duge1169bVhg0bHKoKoWz37t2SVGSb8byGyJObm6thw4bpsssuU4sWLSRZbSUuLk7VqlUrsC9tJfKsXbtW7dq104kTJ1S5cmXNmjVL5557rtasWUMbQZ6PP/5Y33//vVauXFnoNf4+gcfFF1+syZMn6+yzz1ZGRobGjBmj9u3ba926dbQTGwhBAOBHgwcP1rp16wqMywY8zj77bK1Zs0aHDh3S9OnTNXDgQC1atMjpshBCduzYoaFDhyotLU0JCQlOl4MQdvXVV+c9btmypS6++GI1bNhQn3zyiSpUqOBgZeGB4XD/U6tWLUVHRxdaNWPPnj2qV6+eQ1UhlHnaBW0GHkOGDNFnn32mBQsWKCUlJW97vXr1dPLkSR08eLDA/rSVyBMXF6emTZuqdevWGjdunFq1aqUJEybQRpBn1apV2rt3ry688ELFxMQoJiZGixYt0iuvvKKYmBjVrVuXtoIiVatWTWeddZY2b97M3yk2EIL+Jy4uTq1bt9a8efPytuXm5mrevHlq166dg5UhVDVu3Fj16tUr0GYyMzP17bff0mYijDFGQ4YM0axZszR//nw1bty4wOutW7dWbGxsgbayceNGbd++nbYS4XJzc5WVlUUbQZ7OnTtr7dq1WrNmTd6tTZs2GjBgQN5j2gqKcuTIEf36669KSkri7xQbGA6Xz/DhwzVw4EC1adNGbdu21fjx43X06FENGjTI6dLgkCNHjmjz5s15z7ds2aI1a9aoRo0aatCggYYNG6annnpKZ555pho3bqxRo0YpOTlZvXr1cq5oBN3gwYM1ZcoU/fvf/1aVKlXyxltXrVpVFSpUUNWqVXXHHXdo+PDhqlGjhhITE/XXv/5V7dq10yWXXOJw9QiWkSNH6uqrr1aDBg10+PBhTZkyRQsXLtQXX3xBG0GeKlWq5M0n9KhUqZJq1qyZt522AkkaMWKErr32WjVs2FC7du3SE088oejoaN100038nWKH08vThZpXX33VNGjQwMTFxZm2bdua5cuXO10SHLRgwQIjqdBt4MCBxhhrmexRo0aZunXrmvj4eNO5c2ezceNGZ4tG0BXVRiSZd955J2+f48ePm/vuu89Ur17dVKxY0Vx//fUmIyPDuaIRdLfffrtp2LChiYuLM7Vr1zadO3c2X375Zd7rtBEUJ/8S2cbQVmDp16+fSUpKMnFxcaZ+/fqmX79+ZvPmzXmv005K5jLGGIfyFwAAAAAEHXOCAAAAAEQUQhAAAACAiEIIAgAAABBRCEEAAAAAIgohCAAAAEBEIQQBAAAAiCiEIAAAAAARhRAEAAAAIKIQggAAAABEFEIQACCk/P777/rLX/6iBg0aKD4+XvXq1dOVV16pr7/+WpLkcrk0e/ZsZ4sEAIS1GKcLAAAgvz59+ujkyZN69913dcYZZ2jPnj2aN2+e9u/f73RpAIBywmWMMU4XAQCAJB08eFDVq1fXwoUL1aFDh0KvN2rUSNu2bct73rBhQ23dulWS9O9//1tjxozRzz//rOTkZA0cOFCPPfaYYmKsf+9zuVx644039J///EcLFy5UUlKSnn/+efXt2zconw0AEDoYDgcACBmVK1dW5cqVNXv2bGVlZRV6feXKlZKkd955RxkZGXnPlyxZoltvvVVDhw7Vzz//rEmTJmny5Ml6+umnCxw/atQo9enTRz/88IMGDBigG2+8UevXrw/8BwMAhBR6ggAAIWXGjBm66667dPz4cV144YXq0KGDbrzxRrVs2VKS1aMza9Ys9erVK++YLl26qHPnzho5cmTetg8++EAPP/ywdu3alXfcvffeq4kTJ+btc8kll+jCCy/UG2+8EZwPBwAICfQEAQBCSp8+fbRr1y795z//0VVXXaWFCxfqwgsv1OTJk4s95ocfftDYsWPzepIqV66su+66SxkZGTp27Fjefu3atStwXLt27egJAoAIxMIIAICQk5CQoK5du6pr164aNWqU7rzzTj3xxBO67bbbitz/yJEjGjNmjHr37l3kuQAAyI+eIABAyDv33HN19OhRSVJsbKzcbneB1y+88EJt3LhRTZs2LXSLijr1v7rly5cXOG758uU655xzAv8BAAAhhZ4gAEDI2L9/v/785z/r9ttvV8uWLVWlShV99913ev7559WzZ09J1gpx8+bN02WXXab4+HhVr15djz/+uK655ho1aNBAffv2VVRUlH744QetW7dOTz31VN75p02bpjZt2ujyyy/Xhx9+qBUrVuhf//qXUx8XAOAQFkYAAISMrKwsjR49Wl9++aV+/fVXZWdnKzU1VX/+85/16KOPqkKFCvr00081fPhwbd26VfXr189bIvuLL77Q2LFjtXr1asXGxqpZs2a68847ddddd0myFkZ4/fXXNXv2bC1evFhJSUl67rnndMMNNzj4iQEATiAEAQAiQlGrygEAIhNzggAAAABEFEIQAAAAgIjCwggAgIjA6G8AgAc9QQAAAAAiCiEIAAAAQEQhBAEAAACIKIQgAAAAABGFEAQAAAAgohCCAAAAAEQUQhAAAACAiEIIAgAAABBR/h8s0hNkYjwb9gAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Observation\n",
    "\n",
    "The training loss curve exhibits a sharp decline in the initial stages, indicating rapid learning and model adaptation. However, it plateaus after around 10 steps, suggesting that the model may have reached its capacity to learn from the training data. This could be due to overfitting (as we didn't find the same accuracy on the test data), and further tuning of hyperparameters or data augmentation could have improved the model's performance."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
